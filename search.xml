<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AJAX introduction]]></title>
    <url>%2F2018%2F03%2F21%2FAjax-introduction%2F</url>
    <content type="text"><![CDATA[What is AJAX?AJAX stands for Asynchronous JavaScript and XML. AJAX is a new technique for creating better, faster, and more interactive web applications with the help of XML, HTML, CSS, and Java Script. Steps of AJAX Operation A client event occurs. A JavaScript function is called as the result of an event. An XMLHttpRequest object is created. 1234567891011121314151617181920212223var ajaxRequest; // The variable that makes Ajax possible!function ajaxFunction() &#123; try &#123; // Opera 8.0+, Firefox, Safari ajaxRequest = new XMLHttpRequest(); &#125; catch (e) &#123; // Internet Explorer Browsers try &#123; ajaxRequest = new ActiveXObject("Msxml2.XMLHTTP"); &#125; catch (e) &#123; try &#123; ajaxRequest = new ActiveXObject("Microsoft.XMLHTTP"); &#125; catch (e) &#123; // Something went wrong alert("Your browser broke!"); return false; &#125; &#125; &#125;&#125; The XMLHttpRequest object is configured. In this step, we will write a function that will be triggered by the client event and a callback function processRequest() will be registered. 123456789101112function validateUserId() &#123; ajaxFunction(); // Here processRequest() is the callback function. ajaxRequest.onreadystatechange = processRequest; if (!target) target = document.getElementById("userid"); var url = "validate?id=" + escape(target.value); ajaxRequest.open("GET", url, true); ajaxRequest.send(null);&#125; The XMLHttpRequest object makes an asynchronous request to the Webserver. Source code is available in the above piece of code. Code written in bold typeface is responsible to make a request to the webserver. This is all being done using the XMLHttpRequest object ajaxRequest. 123456789101112function validateUserId() &#123; ajaxFunction(); // Here processRequest() is the callback function. ajaxRequest.onreadystatechange = processRequest; if (!target) target = document.getElementById("userid"); var url = "validate?id = " + escape(target.value); ajaxRequest.open("GET", url, true); ajaxRequest.send(null);&#125; The Webserver returns the result containing XML document. Here is the example code using Servlet. 1234567891011121314public void doGet(HttpServletRequest request,HttpServletResponse response) throws IOException, ServletException &#123; String targetId = request.getParameter("id"); if ((targetId != null) &amp;&amp; !accounts.containsKey(targetId.trim())) &#123; response.setContentType("text/xml"); response.setHeader("Cache-Control", "no-cache"); response.getWriter().write("&lt;valid&gt;true&lt;/valid&gt;"); &#125; else &#123; response.setContentType("text/xml"); response.setHeader("Cache-Control", "no-cache"); response.getWriter().write("&lt;valid&gt;false&lt;/valid&gt;"); &#125;&#125; The XMLHttpRequest object calls the callback() function and processes the result. The XMLHttpRequest object was configured to call the processRequest() function when there is a state change to the readyState of the XMLHttpRequest object. Now this function will receive the result from the server and will do the required processing. As in the following example, it sets a variable message on true or false based on the returned value from the Webserver. 123456function processRequest() &#123; if (req.readyState == 4) &#123; if (req.status == 200) &#123; var message = ...; ...&#125; The HTML DOM is updated.This is the final step and in this step, your HTML page will be updated. It happens in the following way: JavaScript gets a reference to any element in a page using DOM API. The recommended way to gain a reference to an element is to call. 1document.getElementById(&quot;userIdMessage&quot;) JavaScript may now be used to modify the element’s attributes; modify the element’s style properties; or add, remove, or modify the child elements. XMLHttpRequestXMLHttpRequest (XHR) is an API that can be used by JavaScript, JScript, VBScript, and other web browser scripting languages to transfer and manipulate XML data to and from a webserver using HTTP, establishing an independent connection channel between a webpage’s Client-Side and Server-Side. XMLHttpRequest Methods abort() Cancels the current request. getAllResponseHeaders() Returns the complete set of HTTP headers as a string. getResponseHeader( headerName ) Returns the value of the specified HTTP header. open( method, URL, async, userName, password ) Specifies the method, URL, and other optional attributes of a request. The method parameter can have a value of “GET”, “POST”, or “HEAD”. Other HTTP methods such as “PUT” and “DELETE” (primarily used in REST applications) may be possible. The “async” parameter specifies whether the request should be handled asynchronously or not. “true” means that the script processing carries on after the send() method without waiting for a response, and “false” means that the script waits for a response before continuing script processing. send( content ) Sends the request. setRequestHeader( label, value ) Adds a label/value pair to the HTTP header to be sent. XMLHttpRequest Properties onreadystatechangeAn event handler for an event that fires at every state change. readyStateThe readyState property defines the current state of the XMLHttpRequest object. readyState = 0After you have created the XMLHttpRequest object, but before you have called the open() method. readyState = 1After you have called the open() method, but before you have called send(). readyState = 2After you have called send(). readyState = 3After the browser has established a communication with the server, but before the server has completed the response. readyState = 4After the request has been completed, and the response data has been completely received from the server. Implementation of AJAXUse Native JavaScriptBased on XMLHttpRequest(XHR) object. Supported by modern web browsers (IE7+、Firefox、Chrome、Safari and Opera). Get XMLHttpRequest object 1var xhr = new XMLHttpRequest(); If compatibility with IE5, IE6 is needed, we can use ActiveX object. 123456789101112var xhr;if (window.XMLHttpRequest) &#123; // Mozilla, Safari...xhr = new XMLHttpRequest();&#125; else if (window.ActiveXObject) &#123; // IEtry &#123;xhr = new ActiveXObject(&apos;Msxml2.XMLHTTP&apos;);&#125; catch (e) &#123;try &#123;xhr = new ActiveXObject(&apos;Microsoft.XMLHTTP&apos;);&#125; catch (e) &#123;&#125;&#125;&#125; Send HTTP request We will open a url and send a request. 1234567891011121314151617// GETvar xhr;if (window.XMLHttpRequest) &#123; // Mozilla, Safari...xhr = new XMLHttpRequest();&#125; else if (window.ActiveXObject) &#123; // IEtry &#123;xhr = new ActiveXObject('Msxml2.XMLHTTP');&#125; catch (e) &#123;try &#123;xhr = new ActiveXObject('Microsoft.XMLHTTP');&#125; catch (e) &#123;&#125;&#125;&#125;if (xhr) &#123;xhr.open('GET', '/api?username=admin&amp;password=root', true);xhr.send(null);&#125; 12345678910111213141516171819// POSTvar xhr;if (window.XMLHttpRequest) &#123; // Mozilla, Safari...xhr = new XMLHttpRequest();&#125; else if (window.ActiveXObject) &#123; // IEtry &#123;xhr = new ActiveXObject('Msxml2.XMLHTTP');&#125; catch (e) &#123;try &#123;xhr = new ActiveXObject('Microsoft.XMLHTTP');&#125; catch (e) &#123;&#125;&#125;&#125;if (xhr) &#123;xhr.open('POST', '/api', true);// Set Content-Type as application/x-www-form-urlencodedxhr.setRequestHeader('Content-Type', 'application/x-www-form-urlencoded');xhr.send('username=admin&amp;password=root');&#125; Process response from server When we send request, we need to specify how to process response from server. 12345678910// Method 1, specify function to callxhr.onreadystatechange = onReadyStateChange;function onReadyStateChange() &#123;// do something&#125;// Method 2, use anonymous functionxhr.onreadystatechange = function()&#123;// do the thing&#125;; Then we can use the readyState property to get the current state. When the value of readyState is 4, which means a complete response has been received, we can now process the response 123456789101112// readyState value// 0 (unsent)// 1 (opened)// 2 (headers_rceived)// 3 (loading)// 4 (done)if (xhr.readyState === 4) &#123;// everything is good, the response is received&#125; else &#123;// still not ready&#125; Complete code example: 1234567891011121314151617181920212223242526272829303132333435363738// POSTvar xhr;if (window.XMLHttpRequest) &#123; // Mozilla, Safari...xhr = new XMLHttpRequest();&#125; else if (window.ActiveXObject) &#123; // IEtry &#123;xhr = new ActiveXObject('Msxml2.XMLHTTP');&#125; catch (e) &#123;try &#123;xhr = new ActiveXObject('Microsoft.XMLHTTP');&#125; catch (e) &#123;&#125;&#125;&#125;if (xhr) &#123;xhr.onreadystatechange = onReadyStateChange;xhr.open('POST', '/api', true);// set Content-Type as application/x-www-form-urlencodedxhr.setRequestHeader('Content-Type', 'application/x-www-form-urlencoded');xhr.send('username=admin&amp;password=root');&#125;// onreadystatechange methodfunction onReadyStateChange() &#123;// the function will be called 4 timesconsole.log(xhr.readyState);if (xhr.readyState === 4) &#123;// everything is good, the response is receivedif (xhr.status === 200) &#123;console.log(xhr.responseText);&#125; else &#123;console.log('There was a problem with the request.');&#125;&#125; else &#123;// still not readyconsole.log('still not ready...');&#125;&#125; We can use onload to replace onreadystatechange = 4 since onload is invoked only when readyState equals 4. 1234567xhr.onload = function () &#123; // invoke onloadif (xhr.status === 200) &#123; // status is 200 means successconsole.log('success');&#125; else &#123;console.log('fail');&#125;&#125; Use JQuery to implement AJAXCode example:12345678$.ajax(&#123;method: 'POST',url: '/api',data: &#123; username: 'admin', password: 'root' &#125;&#125;).done(function(msg) &#123;alert( 'Data Saved: ' + msg );&#125;); The following are two common ways to implement Ajax using JQuery.12345678910111213// GET$.get('/api', function(res) &#123;// do something&#125;);// POSTvar data = &#123;username: 'admin',password: 'root'&#125;;$.post('/api', data, function(res) &#123;// do something&#125;); Use Fetch API to implement AJAXThe Fetch API provides an interface for fetching resources (including across the network). It will seem familiar to anyone who has used XMLHttpRequest, but the new API provides a more powerful and flexible feature set. Let’s see an example of using Fetch API: 1234567fetch('/api').then(function(response) &#123;return response.json();&#125;).then(function(data) &#123;console.log(data);&#125;).catch(function(error) &#123;console.log('Oops, error: ', error);&#125;); Use arrow function of ES6:123fetch('/api').then(response =&gt; response.json()).then(data =&gt; console.log(data)).catch(error =&gt; console.log('Oops, error: ', error)) We can see that it’s simpler after using Fetch API. Complete version:12345678910111213var options = &#123;method: 'POST',headers: &#123;'Accept': 'application/json','Content-Type': 'application/json'&#125;,body: JSON.stringify(&#123; username: 'admin', password: 'root' &#125;),credentials: 'include'&#125;;fetch('/api', options).then(response =&gt; response.json()).then(data =&gt; console.log(data)).catch(error =&gt; console.log('Oops, error: ', error)) ReferenceAJAX TutorialXMLHttpRequest.readyStateFetch API]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>AJAX</tag>
        <tag>JavaScript</tag>
        <tag>JQuery</tag>
        <tag>Fetch API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross-Origin Resource Sharing (CORS) Introduction]]></title>
    <url>%2F2018%2F03%2F20%2FCross-Origin-Resource-Sharing-CORS-Introduction%2F</url>
    <content type="text"><![CDATA[Same-origin PolicyRegular web pages can use the XMLHttpRequest object to send and receive data from remote servers, however they’re restricted in what they can do by the same origin-policy. This is an important concept in the browser security model and dictates that a web browser may only allow scripts on page A to access data on page B if these two pages have the same origin. The origin of a page is defined by its protocol, host and port number. Cross-Origin Resource Sharing (CORS)Cross-Origin Resource Sharing (CORS) is a mechanism that uses additional HTTP headers to let a user agent gain permission to access selected resources from a server on a different origin (domain) than the site currently in use. A user agent makes a cross-origin HTTP request when it requests a resource from a different domain, protocol, or port than the one from which the current document originated. An example of a cross-origin request: A HTML page served from http://domain-a.com makes an &lt;img&gt; src request for http://domain-b.com/image.jpg. Many pages on the web today load resources like CSS stylesheets, images, and scripts from separate domains, such as content delivery networks (CDNs). For security reasons, browsers restrict cross-origin HTTP requests initiated from within scripts. For example, XMLHttpRequest and the Fetch API follow the same-origin policy. This means that a web application using those APIs can only request HTTP resources from the same domain the application was loaded from unless CORS headers are used. Simple requestsSome requests don’t trigger a CORS preflight. Those are called “simple requests” in this article, though the Fetch spec (which defines CORS) doesn’t use that term. A request that doesn’t trigger a CORS preflight—a so-called “simple request”—is one that meets all the following conditions: The only allowed methods are: GET HEAD POST Apart from the headers set automatically by the user agent (for example, Connection, User-Agent, or any of the other headers with names defined in the Fetch spec as a “forbidden header name”), the only headers which are allowed to be manually set are those which the Fetch spec defines as being a “CORS-safelisted request-header”, which are: Accept Accept-Language Content-Language Content-Type (but note the additional requirements below) Last-Event-ID DPR Save-Data Viewport-Width Width The only allowed values for the Content-Type header are: application/x-www-form-urlencoded multipart/form-data text/plain For example, suppose web content on domain http://foo.example wishes to invoke content on domain http://bar.other. Code of this sort might be used within JavaScript deployed on foo.example: 12345678910var invocation = new XMLHttpRequest();var url = 'http://bar.other/resources/public-data/';function callOtherDomain() &#123; if(invocation) &#123; invocation.open('GET', url, true); invocation.onreadystatechange = handler; invocation.send(); &#125;&#125; This will lead to a simple exchange between the client and the server, using CORS headers to handle the privileges: Let us look at what the browser will send to the server in this case, and let’s see how the server responds: 12345678910111213141516171819202122GET /resources/public-data/ HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1b3pre) Gecko/20081130 Minefield/3.1b3preAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Connection: keep-aliveReferer: http://foo.example/examples/access-control/simpleXSInvocation.htmlOrigin: http://foo.exampleHTTP/1.1 200 OKDate: Mon, 01 Dec 2008 00:23:53 GMTServer: Apache/2.0.61Access-Control-Allow-Origin: *Keep-Alive: timeout=2, max=100Connection: Keep-AliveTransfer-Encoding: chunkedContent-Type: application/xml[XML Data] Lines 1 - 10 are headers sent. The main HTTP request header of note here is the Origin header on line 10 above, which shows that the invocation is coming from content on the domain http://foo.example. Lines 13 - 22 show the HTTP response from the server on domain http://bar.other. In response, the server sends back an Access-Control-Allow-Origin header, shown above in line 16. The Access-Control-Allow-Origin response header indicates whether the response can be shared with resources with the given origin. The use of the Origin header and of Access-Control-Allow-Origin show the access control protocol in its simplest use. In this case, the server responds with a Access-Control-Allow-Origin: * which means that the resource can be accessed by any domain in a cross-site manner. If the resource owners at http://bar.other wished to restrict access to the resource to requests only from http://foo.example, they would send back: Access-Control-Allow-Origin: http://foo.example Note that now, no domain other than http://foo.example (identified by the ORIGIN: header in the request, as in line 10 above) can access the resource in a cross-site manner. The Access-Control-Allow-Origin header should contain the value that was sent in the request’s Origin header. Preflighted requestsUnlike “simple requests” (discussed above), “preflighted” requests first send an HTTP request by the OPTIONS method to the resource on the other domain, in order to determine whether the actual request is safe to send. Cross-site requests are preflighted like this since they may have implications to user data. The HTTP OPTIONS method is used to describe the communication options for the target resource. The client can specify a URL for the OPTIONS method, or an asterisk (*) to refer to the entire server. In particular, a request is preflighted if any of the following conditions is true: If the request uses any of the following methods: PUT DELETE CONNECT OPTIONS TRACE PATCH Or if, apart from the headers set automatically by the user agent (for example, Connection, User-Agent, or any of the other header with a name defined in the Fetch spec as a “forbidden header name”), the request includes any headers other than those which the Fetch spec defines as being a “CORS-safelisted request-header”, which are the following: Accept Accept-Language Content-Language Content-Type (but note the additional requirements below) Last-Event-ID DPR Save-Data Viewport-Width Width Or if the Content-Type header has a value other than the following: application/x-www-form-urlencoded multipart/form-data text/plain Or if one or more event listeners are registered on an XMLHttpRequestUpload object used in the request. The following is an example of a request that will be preflighted. 12345678910111213141516var invocation = new XMLHttpRequest();var url = 'http://bar.other/resources/post-here/';var body = '&lt;?xml version="1.0"?&gt;&lt;person&gt;&lt;name&gt;Arun&lt;/name&gt;&lt;/person&gt;';function callOtherDomain()&#123; if(invocation) &#123; invocation.open('POST', url, true); invocation.setRequestHeader('X-PINGOTHER', 'pingpong'); invocation.setRequestHeader('Content-Type', 'application/xml'); invocation.onreadystatechange = handler; invocation.send(body); &#125;&#125;...... In the example above, line 3 creates an XML body to send with the POST request in line 8. Also, on line 9, a “customized” (non-standard) HTTP request header is set (X-PINGOTHER: pingpong). Such headers are not part of the HTTP/1.1 protocol, but are generally useful to web applications. Since the request uses a Content-Type of application/xml, and since a custom header is set, this request is preflighted. Let’s take a look at the full exchange between client and server. The first exchange is the preflight request/response: 1234567891011121314151617181920212223242526OPTIONS /resources/post-here/ HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1b3pre) Gecko/20081130 Minefield/3.1b3preAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Connection: keep-aliveOrigin: http://foo.exampleAccess-Control-Request-Method: POSTAccess-Control-Request-Headers: X-PINGOTHER, Content-TypeHTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://foo.exampleAccess-Control-Allow-Methods: POST, GET, OPTIONSAccess-Control-Allow-Headers: X-PINGOTHER, Content-TypeAccess-Control-Max-Age: 86400Vary: Accept-Encoding, OriginContent-Encoding: gzipContent-Length: 0Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain Once the preflight request is complete, the real request is sent: 12345678910111213141516171819202122232425262728293031POST /resources/post-here/ HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1b3pre) Gecko/20081130 Minefield/3.1b3preAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Connection: keep-aliveX-PINGOTHER: pingpongContent-Type: text/xml; charset=UTF-8Referer: http://foo.example/examples/preflightInvocation.htmlContent-Length: 55Origin: http://foo.examplePragma: no-cacheCache-Control: no-cache&lt;?xml version="1.0"?&gt;&lt;person&gt;&lt;name&gt;Arun&lt;/name&gt;&lt;/person&gt;HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:40 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://foo.exampleVary: Accept-Encoding, OriginContent-Encoding: gzipContent-Length: 235Keep-Alive: timeout=2, max=99Connection: Keep-AliveContent-Type: text/plain[Some GZIP'd payload] Lines 1 - 12 above represent the preflight request with the OPTIONS method. The browser determines that it needs to send this based on the request parameters that the JavaScript code snippet above was using, so that the server can respond whether it is acceptable to send the request with the actual request parameters. OPTIONS is an HTTP/1.1 method that is used to determine further information from servers, and is a safe method, meaning that it can’t be used to change the resource. Note that along with the OPTIONS request, two other request headers are sent (lines 10 and 11 respectively): 12Access-Control-Request-Method: POSTAccess-Control-Request-Headers: X-PINGOTHER, Content-Type The Access-Control-Request-Method header notifies the server as part of a preflight request that when the actual request is sent, it will be sent with a POST request method. The Access-Control-Request-Method request header is used when issuing a preflight request to let the server know which HTTP method will be used when the actual request is made. This header is necessary as the preflight request is always an OPTIONS and doesn’t use the same method as the actual request. The Access-Control-Request-Headers header notifies the server that when the actual request is sent, it will be sent with a X-PINGOTHER and Content-Type custom headers. The server now has an opportunity to determine whether it wishes to accept a request under these circumstances. The Access-Control-Request-Headers request header is used when issuing a preflight request to let the server know which HTTP headers will be used when the actual request is made. Lines 14 - 26 above are the response that the server sends back indicating that the request method (POST) and request headers (X-PINGOTHER) are acceptable. In particular, let’s look at lines 17-20: 1234Access-Control-Allow-Origin: http://foo.exampleAccess-Control-Allow-Methods: POST, GET, OPTIONSAccess-Control-Allow-Headers: X-PINGOTHER, Content-TypeAccess-Control-Max-Age: 86400 The server responds with Access-Control-Allow-Methods and says that POST, GET, and OPTIONS are viable methods to query the resource in question. Note that this header is similar to the Allow response header, but used strictly within the context of access control. The Access-Control-Allow-Methods response header specifies the method or methods allowed when accessing the resource in response to a preflight request. The server also sends Access-Control-Allow-Headers with a value of “X-PINGOTHER, Content-Type”, confirming that these are permitted headers to be used with the actual request. Like Access-Control-Allow-Methods, Access-Control-Allow-Headers is a comma separated list of acceptable headers. The Access-Control-Allow-Headers response header is used in response to a preflight request to indicate which HTTP headers can be used during the actual request. Finally, Access-Control-Max-Age gives the value in seconds for how long the response to the preflight request can be cached for without sending another preflight request. In this case, 86400 seconds is 24 hours. Note that each browser has a maximum internal value that takes precedence when the Access-Control-Max-Age is greater. Requests with credentialsThe most interesting capability exposed by both XMLHttpRequest or Fetch and CORS is the ability to make “credentialed” requests that are aware of HTTP cookies and HTTP Authentication information. By default, in cross-site XMLHttpRequest or Fetch invocations, browsers will not send credentials. A specific flag has to be set on the XMLHttpRequest object or the Request constructor when it is invoked. Use XMLHttpRequest (XHR) objects to interact with servers. You can retrieve data from a URL without having to do a full page refresh. This enables a Web page to update just part of a page without disrupting what the user is doing. XMLHttpRequest is used heavily in Ajax programming. The Fetch API provides an interface for fetching resources (including across the network). It will seem familiar to anyone who has used XMLHttpRequest, but the new API provides a more powerful and flexible feature set. In this example, content originally loaded from http://foo.example makes a simple GET request to a resource on http://bar.other which sets Cookies. Content on foo.example might contain JavaScript like this: 1234567891011var invocation = new XMLHttpRequest();var url = 'http://bar.other/resources/credentialed-content/';function callOtherDomain()&#123; if(invocation) &#123; invocation.open('GET', url, true); invocation.withCredentials = true; invocation.onreadystatechange = handler; invocation.send(); &#125;&#125; Line 7 shows the flag on XMLHttpRequest that has to be set in order to make the invocation with Cookies, namely the withCredentials boolean value. By default, the invocation is made without Cookies. Since this is a simple GET request, it is not preflighted, but the browser will reject any response that does not have the Access-Control-Allow-Credentials: true header, and not make the response available to the invoking web content. Here is a sample exchange between client and server: 12345678910111213141516171819202122232425262728293031GET /resources/access-control-with-credentials/ HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1b3pre) Gecko/20081130 Minefield/3.1b3preAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Connection: keep-aliveReferer: http://foo.example/examples/credential.htmlOrigin: http://foo.exampleCookie: pageAccess=2HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:34:52 GMTServer: Apache/2.0.61 (Unix) PHP/4.4.7 mod_ssl/2.0.61 OpenSSL/0.9.7e mod_fastcgi/2.4.2 DAV/2 SVN/1.4.2X-Powered-By: PHP/5.2.6Access-Control-Allow-Origin: http://foo.exampleAccess-Control-Allow-Credentials: trueCache-Control: no-cachePragma: no-cacheSet-Cookie: pageAccess=3; expires=Wed, 31-Dec-2008 01:34:53 GMTVary: Accept-Encoding, OriginContent-Encoding: gzipContent-Length: 106Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain[text/plain payload] Although line 11 contains the Cookie destined for the content on http://bar.other, if bar.other did not respond with an Access-Control-Allow-Credentials: true (line 19) the response would be ignored and not made available to web content. Credentialed requests and wildcardsWhen responding to a credentialed request, the server must specify an origin in the value of the Access-Control-Allow-Origin header, instead of specifying the &quot;*&quot; wildcard. Because the request headers in the above example include a Cookie header, the request would fail if the value of the Access-Control-Allow-Origin header were &quot;*&quot;. But it does not fail: Because the value of the Access-Control-Allow-Origin header is “http://foo.example“ (an actual origin) rather than the &quot;*&quot; wildcard, the credential-cognizant content is returned to the invoking web content. Compatibility notesInternet Explorer 8 and 9 expose CORS via the XDomainRequest object, but have a full implementation in IE 10. While Firefox 3.5 introduced support for cross-site XMLHttpRequests and Web Fonts, certain requests were limited until later versions. Specifically, Firefox 7 introduced the ability for cross-site HTTP requests for WebGL Textures, and Firefox 9 added support for Images drawn on a canvas using drawImage. ReferenceCross-Origin Resource Sharing (CORS)Cross-Origin Resource Sharing – W3Same-origin policyHTTP OPTIONSAccess-Control-Allow-OriginAccess-Control-Allow-HeadersAccess-Control-Allow-MethodsXMLHttpRequestFetch APIStack Overflow answer with “how to” info for dealing with common problems]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>CORS</tag>
        <tag>HTTP</tag>
        <tag>XMLHttpRequest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch normalization in Neural Networks]]></title>
    <url>%2F2018%2F03%2F15%2FBatch-normalization%2F</url>
    <content type="text"><![CDATA[Why do we use batch normalization?We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed. How does batch normalization work?To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. However, after this shift/scale of activation outputs by some randomly initialized parameters, the weights in the next layer are no longer optimal. SGD ( Stochastic gradient descent) undoes this normalization if it’s a way for it to minimize the loss function. Consequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a “standard deviation” parameter (gamma) and add a “mean” parameter (beta). In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights. Followings are the properties of Batch Normalization with mean and variance for a mini batch version: Learning faster: Learning rate can be increased compare to non-batch-normalized version. Increase Accuracy: Flexibility on mean and variance value for every dimension in every hidden layer provides better learning, hence accuracy of the network. Normalization or Whitening of the inputs to each layer: Zero means, unit variances and or not decorrelated. To remove the ill-effect of Internal Covariate shift:Transformation makes data to big or to small; change of the input distribution away from normalization due to successive transformation. Not-Stuck in the saturation mode: Even if ReLU is not used. Integrate Whitening within the gradient descent optimization: Decoupled Whitening between training steps, which modifies network directly, reduces the effort of optimization. So, model blows up when normalization parameters are computed outside the gradient descent step. Whitening within gradient descent: Requires inverse square root of covariance matrix as well as derivatives for backpropagation Normalization of Individual dimension: Individual dimension of hidden layers are normalized independently rather than joint covariances. So, features are not decorrelated. Normalization of mini-batch: Estimation of mean and variance are computed after each mini-batch rather than entire training set. Even ignoring the joint covariance as it will create singular co-variance matrices for such small number of training sample per mini-batch compare to high dimension size of the hidden layer. Learning of scale and shift for every dimension: Scaled and shifted values are passed to the next layer, whether mean and variances are calculated after getting all mini-batch activation of current layer. So, forward pass of all the samples within the mini-batch should pass layer wise. Backpropagation is required for getting gradient of weights as well as scaling (variance) and shift (mean). Inference: During inference moving averaged mean and variance parameters during mini batch training are considered. Convolution Neural Network: Whitening of intermediate layers, before or after the nonlinearity creates a lot of new innovation pathways [11-15]. ReferenceBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate ShiftBatch normalization in Neural Networks]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Batch normalization</tag>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Front-end Handbook HTML]]></title>
    <url>%2F2018%2F03%2F09%2FFront-end-Handbook-HTML%2F</url>
    <content type="text"><![CDATA[Describe the difference between a cookie, sessionStorage and localStorage / cookie localStorage sessionStorage Initiator Client or server. Server can use Set-Cookie header Client Client Expiry Manually set Forever On tab close Persistent across browser sessions Depends on whether expiration is set Yes No Have domain associated Yes No No Sent to server with every HTTP request Cookies are automatically being sent via Cookie header No No Capacity (per domain) 4kb 5MB 5MB Accessibility Any window Any window Same tab Referenceshttps://developer.mozilla.org/en-US/docs/Web/HTTP/Cookieshttp://tutorial.techaltum.com/local-and-session-storage.html Can you explain CSS box model?CSS box model is a rectangular space around a HTML element which defines border, padding and margin. Border: - This defines the maximum area in which the element will be contained. We can make the border visible, invisible, define height and width etc. Padding: - This defines the spacing between border and element. Margin: - This defines the spacing between border and any neighboring elements. What are web sockets?Web Sockets is a next-generation bidirectional communication technology for web applications which operates over a single socket and is exposed via a JavaScript interface in HTML 5 compliant browsers. Once you get a Web Socket connection with the web server, you can send data from browser to server by calling a send() method, and receive data from server to browser by an onmessage event handler. Following is the API which creates a new WebSocket object.1var Socket = new WebSocket(url, [protocal] ); Here first argument, url, specifies the URL to which to connect. The second attribute, protocol is optional, and if present, specifies a sub-protocol that the server must support for the connection to be successful. Cross-Origin Resource Sharing (CORS)Cross-Origin Resource Sharing (CORS) is a mechanism that uses additional HTTP headers to let a user agent gain permission to access selected resources from a server on a different origin (domain) than the site currently in use. A user agent makes a cross-origin HTTP request when it requests a resource from a different domain, protocol, or port than the one from which the current document originated. An example of a cross-origin request: A HTML page served from http://domain-a.com makes an &lt;img&gt; src request for http://domain-b.com/image.jpg. Many pages on the web today load resources like CSS stylesheets, images, and scripts from separate domains, such as content delivery networks (CDNs). For security reasons, browsers restrict cross-origin HTTP requests initiated from within scripts. For example, XMLHttpRequest and the Fetch API follow the same-origin policy. This means that a web application using those APIs can only request HTTP resources from the same domain the application was loaded from unless CORS headers are used. HTTP Status Codes 1xx Informational 2xx Success 200 OK 201 Created 204 No Content 3xx Redirection 304 Not Modified 4xx Client Error 400 Bad Request 401 Unauthorized 403 Forbidden 404 Not Found 409 Conflict 5xx Server Error 500 Internal Server Error Referenceshttp://www.restapitutorial.com/httpstatuscodes.html]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Glossary]]></title>
    <url>%2F2018%2F03%2F09%2FMachine-Learning-Glossary%2F</url>
    <content type="text"><![CDATA[Aactivation functionA function (for example, ReLU or sigmoid) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer. AdaGradA sophisticated gradient descent algorithm that rescales the gradients of each parameter, effectively giving each parameter an independent learning rate. For a full explanation, see this paper. AUC (Area under the ROC Curve)An evaluation metric that considers all possible classification thresholds. The Area Under the ROC curve is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive. BbackpropagationThe primary algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated (and cached) in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph. batchThe set of examples used in one iteration (that is, one gradient update) of model training. biasAn error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). Compare with variance. bias–variance decompositionA way of analyzing a learning algorithm’s expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself. binary classificationA type of classification task that outputs one of two mutually exclusive classes. For example, a machine learning model that evaluates email messages and outputs either “spam” or “not spam” is a binary classifier. bucketingConverting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin. Ccross-entropyA generalization of Log Loss to multi-class classification problems. Cross-entropy quantifies the difference between two probability distributions. See also perplexity. Ddropout regularizationA form of regularization useful in training neural networks. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. For full details, see Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Eearly stoppingA method for regularization that involves ending model training before training loss finishes decreasing. In early stopping, you end model training when the loss on a validation data set starts to increase, that is, when generalization performance worsens. embeddingsA categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways: As a million-element (high-dimensional) sparse vector in which all elements are integers. Each cell in the vector represents a separate English word; the value in a cell represents the number of times that word appears in a sentence. Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0. The few cells that aren’t 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence. As a several-hundred-element (low-dimensional) dense vector in which each element holds a floating-point value between 0 and 1. This is an embedding. In TensorFlow, embeddings are trained by backpropagating loss just like any other parameter in a neural network. empirical risk minimization (ERM)Choosing the model function that minimizes loss on the training set. Contrast with structural risk minimization. ensembleA merger of the predictions of multiple models. You can create an ensemble via one or more of the following: different initializations different hyperparameters different overall structure Deep and wide models are a kind of ensemble. EpochAn Epoch is a complete pass through all the training data. A neural network is trained until the error rate is acceptable, and this will often take multiple passes through the complete data set. note An iteration is when parameters are updated and is typically less than a full pass. For example if BatchSize is 100 and data size is 1,000 an epoch will have 10 iterations. If trained for 30 epochs there will be 300 iterations. Ffully connected layerA hidden layer in which each node is connected to every node in the subsequent hidden layer. A fully connected layer is also known as a dense layer. Ggradient descentThe gradient is a derivative, which you will know from differential calculus. That is, it’s the ratio of the rate of change of a neural net’s parameters and the error it produces, as it learns how to reconstruct a dataset or make guesses about labels. The process of minimizing error is called gradient descent. Descending a gradient has two aspects: choosing the direction to step in (momentum) and choosing the size of the step (learning rate). HheuristicA practical and nonoptimal solution to a problem, which is sufficient for making progress or for learning from. hidden layerA synthetic layer in a neural network between the input layer (that is, the features) and the output layer (the prediction). A neural network contains one or more hidden layers. hinge lossA family of loss functions for classification designed to find the decision boundary as distant as possible from each training example, thus maximizing the margin between examples and the boundary. KSVMs use hinge loss (or a related function, such as squared hinge loss). For binary classification, the hinge loss function is defined as follows: $$ loss = max(0, 1-(y’*y)) $$ where y’ is the raw output of the classifier model: $$ y’ = b + w_1x_1 + w_2x_2 + … + w_nx_n $$ and y is the true label, either -1 or +1. hyperparameterThe “knobs” that you tweak during successive runs of training a model. For example, learning rate is a hyperparameter. Contrast with parameter. hyperplane“A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts. What does that mean intuitively? First think of the real line. Now pick a point. That point divides the real line into two parts (the part above that point, and the part below that point). The real line has 1 dimension, while the point has 0 dimensions. So a point is a hyperplane of the real line. Now think of the two-dimensional plane. Now pick any line. That line divides the plane into two parts (“left” and “right” or maybe “above” and “below”). The plane has 2 dimensions, but the line has only one. So a line is a hyperplane of the 2d plane. Notice that if you pick a point, it doesn’t divide the 2d plane into two parts. So one point is not enough. Now think of a 3d space. Now to divide the space into two parts, you need a plane. Your plane has two dimensions, your space has three. So a plane is the hyperplane for a 3d space. OK, now we’ve run out of visual examples. But suppose you have a space of n dimensions. You can write down an equation describing an n-1 dimensional object that divides the n-dimensional space into two pieces. That’s a hyperplane.” Iinput layerThe first layer (the one that receives the input data) in a neural network. KKerasA popular Python machine learning API. Keras runs on several deep learning frameworks, including TensorFlow, where it is made available as tf.keras. Kernel Support Vector Machines (KSVMs)A classification algorithm that seeks to maximize the margin between positive and negative classes by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input data set consists of a hundred features. In order to maximize the margin between positive and negative classes, KSVMs could internally map those features into a million-dimension space. KSVMs uses a loss function called hinge loss. LL1 lossLoss function based on the absolute value of the difference between the values that a model is predicting and the actual values of the labels. L1 loss is less sensitive to outliers than L2 loss. L1 regularizationA type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with L2 regularization. L2 lossSee squared loss. L2 regularizationA type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models. learning rateA scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step. Learning rate is a key hyperparameter. least squares regressionA linear regression model trained by minimizing L2 Loss. linear regressionA type of regression model that outputs a continuous value from a linear combination of input features. logistic regressionA model that generates a probability for each possible discrete label value in classification problems by applying a sigmoid function to a linear prediction. Although logistic regression is often used in binary classification problems, it can also be used in multi-class classification problems (where it becomes called multi-class logistic regression or multinomial regression). log-likelihoodLog likelihood is related to the statistical idea of the likelihood function. Likelihood is a function of the parameters of a statistical model. “The probability of some observed outcomes given a set of parameter values is referred to as the likelihood of the set of parameter values given the observed outcomes.” lossA measure of how far a model’s predictions are from its label. Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use mean squared error for a loss function, while logistic regression models use Log Loss. Hinge Loss (SVM) Cross Entropy Loss (Logistic Regression) Softmax Loss (Softmax) Square Loss (OLS) Exponential Loss (Adaboost) 0-1 Loss, Absolute Loss… Mmachine learningA program or system that builds (trains) a predictive model from input data. The system uses the learned model to make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems. Mean Squared Error (MSE)The average squared loss per example. MSE is calculated by dividing the squared loss by the number of examples. The values that TensorFlow Playground displays for “Training loss” and “Test loss” are MSE. mini-batchA small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference. The batch size of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data. mini-batch stochastic gradient descent (SGD)A gradient descent algorithm that uses mini-batches. In other words, mini-batch SGD estimates the gradient based on a small subset of the training data. Vanilla SGD uses a mini-batch of size 1. MNISTMNIST is the “hello world” of deep-learning datasets. Everyone uses MNIST to test their neural networks, just to see if the net actually works at all. MNIST contains 60,000 training examples and 10,000 test examples of the handwritten numerals 0-9. These images are 28x28 pixels, which means they require 784 nodes on the first input layer of a neural network. MNIST is available for download here. Here is an example of training a DBN on MNIST with Deeplearning4j. MomentumA sophisticated gradient descent algorithm in which a learning step depends not only on the derivative in the current step, but also on the derivatives of the step(s) that immediately preceded it. Momentum involves computing an exponentially weighted moving average of the gradients over time, analogous to momentum in physics. Momentum sometimes prevents learning from getting stuck in local minima. multi-class classificationClassification problems that distinguish among more than two classes. For example, there are approximately 128 species of maple trees, so a model that categorized maple tree species would be multi-class. Conversely, a model that divided emails into only two categories (spam and not spam) would be a binary classification model. NNaN trapWhen one number in your model becomes a NaN during training, which causes many or all other numbers in your model to eventually become a NaN. NaN is an abbreviation for “Not a Number.” neural networkA model that, taking inspiration from the brain, is composed of layers (at least one of which is hidden) consisting of simple connected units or neurons followed by nonlinearities. neuronA node in a neural network, typically taking in multiple input values and generating one output value. The neuron calculates the output value by applying an activation function (nonlinear transformation) to a weighted sum of input values. nodeAn overloaded term that means either of the following: A neuron in a hidden layer. An operation in a TensorFlow graph. normalizationThe process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. For example, suppose the natural range of a certain feature is 800 to 6,000. Through subtraction and division, you can normalize those values into the range -1 to +1. See also scaling. OoptimizerA specific implementation of the gradient descent algorithm. TensorFlow’s base class for optimizers is tf.train.Optimizer. Different optimizers may leverage one or more of the following concepts to enhance the effectiveness of gradient descent on a given training set: momentum (Momentum) update frequency (AdaGrad = ADAptive GRADient descent; Adam = ADAptive with Momentum; RMSProp) sparsity/regularization (Ftrl) more complex math (Proximal, and others) You might even imagine an NN-driven optimizer. outliersValues distant from most other values. In machine learning, any of the following are outliers: Weights with high absolute values. Predicted values relatively far away from the actual values. Input data whose values are more than roughly 3 standard deviations from the mean. Outliers often cause problems in model training. output layerThe “final” layer of a neural network. The layer containing the answer(s). overfittingCreating a model that matches the training data so closely that the model fails to make correct predictions on new data. PparameterA variable of a model that the ML system trains on its own. For example, weights are parameters whose values the ML system gradually learns through successive training iterations. Contrast with hyperparameter. partial derivativeA derivative in which all but one of the variables is considered a constant. For example, the partial derivative of f(x, y) with respect to x is the derivative of f considered as a function of x alone (that is, keeping y constant). The partial derivative of f with respect to x focuses only on how x is changing and ignores all other variables in the equation. performanceOverloaded term with the following meanings: The traditional meaning within software engineering. Namely: How fast (or efficiently) does this piece of software run? The meaning within ML. Here, performance answers the following question: How correct is this model? That is, how good are the model’s predictions? perplexityOne measure of how well a model is accomplishing its task. For example, suppose your task is to read the first few letters of a word a user is typing on a smartphone keyboard, and to offer a list of possible completion words. Perplexity, P, for this task is approximately the number of guesses you need to offer in order for your list to contain the actual word the user is trying to type. Perplexity is related to cross-entropy as follows:$$ P = 2^{-cross entropy} $$ pipelineThe infrastructure surrounding a machine learning algorithm. A pipeline includes gathering the data, putting the data into training data files, training one or more models, and exporting the models to production. poolingPooling, max pooling and average pooling are terms that refer to downsampling or subsampling within a convolutional network. Downsampling is a way of reducing the amount of data flowing through the network, and therefore decreasing the computational cost of the network. Average pooling takes the average of several values. Max pooling takes the greatest of several values. Max pooling is currently the preferred type of downsampling layer in convolutional networks. precisionA metric for classification models. Precision identifies the frequency with which a model was correct when predicting the positive class. That is:$$ Precision = \frac{True Positives}{True Positives + False Positives} $$ RrecallA metric for classification models that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is:$$ Recall = \frac{True Positives}{True Positives + False Negatives} $$ Rectified Linear Unit (ReLU)An activation function with the following rules: If input is negative or zero, output is 0. If input is positive, output is equal to input. regression modelA type of model that outputs continuous (typically, floating-point) values. Compare with classification models, which output discrete values, such as “day lily” or “tiger lily.” regularizationThe penalty on a model’s complexity. Regularization helps prevent overfitting. Different kinds of regularization include: L1 regularization L2 regularization dropout regularization early stopping (this is not a formal regularization method, but can effectively limit overfitting) regularization rateA scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate’s influence: $$ minimize(loss function + \lambda(regularization function))$$ Raising the regularization rate reduces overfitting but may make the model less accurate. ROC (receiver operating characteristic) CurveA curve of true positive rate vs. false positive rate at different classification thresholds. See also AUC. SscalingA commonly used practice in feature engineering to tame a feature’s range of values to match the range of other features in the data set. For example, suppose that you want all floating-point features in the data set to have a range of 0 to 1. Given a particular feature’s range of 0 to 500, you could scale that feature by dividing each value by 500. See also normalization. sigmoid functionA function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1. The sigmoid function has the following formula: $$ y = \frac{1}{1+e^{-\sigma}} $$ where $\sigma$ in logistic regression problems is simply: $$\sigma = b + w_1x_1 + w_2x_2 + … + w_nx_n$$ In other words, the sigmoid function converts $\sigma$ into a probability between 0 and 1. In some neural networks, the sigmoid function acts as the activation function. softmaxA function that provides probabilities for each possible class in a multi-class classification model. The probabilities add up to exactly 1.0. For example, softmax might determine that the probability of a particular image being a dog at 0.9, a cat at 0.08, and a horse at 0.02. (Also called full softmax.) Contrast with candidate sampling. sparse featureFeature vector whose values are predominately zero or empty. For example, a vector containing a single 1 value and a million 0 values is sparse. As another example, words in a search query could also be a sparse feature—there are many possible words in a given language, but only a few of them occur in a given query. Contrast with dense feature. stochastic gradient descent (SGD)A gradient descent algorithm in which the batch size is one. In other words, SGD relies on a single example chosen uniformly at random from a data set to calculate an estimate of the gradient at each step. structural risk minimization (SRM)An algorithm that balances two goals: The desire to build the most predictive model (for example, lowest loss).The desire to keep the model as simple as possible (for example, strong regularization).For example, a model function that minimizes loss+regularization on the training set is a structural risk minimization algorithm. For more information, see http://www.svms.org/srm/. Contrast with empirical risk minimization. supervised machine learningTraining a model from input data and its corresponding labels. Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers. After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with unsupervised machine learning. TTensorThe primary data structure in TensorFlow programs. Tensors are N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values. TensorFlowA large-scale, distributed, machine learning platform. The term also refers to the base API layer in the TensorFlow stack, which supports general computation on dataflow graphs. Although TensorFlow is primarily used for machine learning, you may also use TensorFlow for non-ML tasks that require numerical computation using dataflow graphs. time series analysisA subfield of machine learning and statistics that analyzes temporal data. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data. trainingThe process of determining the ideal parameters comprising a model. training setThe subset of the data set used to train a model. Contrast with validation set and test set. transfer learningTransferring information from one machine learning task to another. For example, in multi-task learning, a single model solves multiple tasks, such as a deep model that has different output nodes for different tasks. Transfer learning might involve transferring knowledge from the solution of a simpler task to a more complex one, or involve transferring knowledge from a task where there is more data to one where there is less data. Most machine learning systems solve a single task. Transfer learning is a baby step towards artificial intelligence in which a single program can solve multiple tasks. Uunsupervised machine learningTraining a model to find patterns in a data set, typically an unlabeled data set. The most common use of unsupervised machine learning is to cluster data into groups of similar examples. For example, an unsupervised machine learning algorithm can cluster songs together based on various properties of the music. The resulting clusters can become an input to other machine learning algorithms (for example, to a music recommendation service). Clustering can be helpful in domains where true labels are hard to obtain. For example, in domains such as anti-abuse and fraud, clusters can help humans better understand the data. Another example of unsupervised machine learning is principal component analysis (PCA). For example, applying PCA on a data set containing the contents of millions of shopping carts might reveal that shopping carts containing lemons frequently also contain antacids. Compare with supervised machine learning. VvarianceAn error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). Compare with bias. Wwide modelA linear model that typically has many sparse input features. We refer to it as “wide” since such a model is a special type of neural network with a large number of inputs that connect directly to the output node. Wide models are often easier to debug and inspect than deep models. Although wide models cannot express nonlinearities through hidden layers, they can use transformations such as feature crossing and bucketization to model nonlinearities in different ways. Contrast with deep model.]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TypeScript Introduction]]></title>
    <url>%2F2018%2F03%2F01%2FTypeScript-Introduction%2F</url>
    <content type="text"><![CDATA[What is TypeScriptBy definition, “TypeScript is JavaScript for application-scale development.” TypeScript is a strongly typed, object oriented, compiled language. It was designed by Anders Hejlsberg (designer of C#) at Microsoft. TypeScript is both a language and a set of tools. TypeScript is a typed superset of JavaScript compiled to JavaScript. In other words, TypeScript is JavaScript plus some additional features. PrerequisitesTo use TypeScript, you should have a good understanding of OOP concepts and basic JavaScript, to make the most of this tutorial. Relation to JavaScriptJavaScript is standardized through the ECMAScript standards. Not all browsers in use support all features of newer ECMAScript standards (see this table). TypeScript supports new ECMAScript standards and compiles them to (older) ECMAScript targets of your choosing (current targets are 3, 5 and 6 [a.k.a. 2015]). This means that you can use features of ES2015 and beyond, like modules, lambda functions, classes, the spread operator, destructuring, today. It also adds type support of course, which is not part of any ECMAScript standard and may likely never be due to the interpreted nature instead of compiled nature of JavaScript. The type system of TypeScript is relatively rich and includes: interfaces, enums, hybrid types, generics, union and intersection types, access modifiers and much more. The official website of TypeScript gives an overview of these features. Are there other technologies like it?There’s CoffeeScript, but that really serves a different purpose. IMHO, CoffeeScript provides readability for humans, but TypeScript also provides deep readability for tools through its optional static typing. There’s also Dart but that’s a full on replacement for JavaScript (though it can produce JavaScript code). FeaturesOptionally static typing and type inferenceJavaScript is dynamically typed. This means JavaScript does not know what type a variable is until it is actually instantiated at run-time. This also means that it may be too late. TypeScript adds type support to JavaScript. Bugs that are caused by false assumptions of some variable being of a certain type can be completely eradicated if you play your cards right; how strict you type your code or if you type your code at all is up to you. TypeScript makes typing a bit easier and a lot less explicit by the usage of type inference. For example: var x = “hello” in TypeScript is the same as var x : string = “hello”. The type is simply inferred from its use. Even it you don’t explicitly type the types, they are still there to save you from doing something which otherwise would result in a run-time error. TypeScript is optionally typed by default. For example function divideByTwo(x) { return x / 2 } is a valid function in TypeScript which can be called with any kind of parameter, even though calling it with a string will obviously result in a runtime error. Just like you are used to in JavaScript. This works, because when no type was explicitly assigned and the type could not be inferred, like in the divideByTwo example, TypeScript will implicitly assign the type any. This means the divideByTwo function’s type signature automatically becomes function divideByTwo(x : any) : any There is a compiler flag to disallow this behavior: --noImplicitAny. Enabling this flag gives you a greater degree of safety, but also means you will have to do more typing. InterfacesThe interface keyword is used to declare an interface. Here is the syntax to declare an interface Declaring InterfacesSyntax12interface interface_name &#123;&#125; Example: Interface and Objects12345678910111213141516171819202122232425interface IPerson &#123;firstName:string,lastName:string,sayHi: ()=&gt;string&#125;var customer:IPerson = &#123;firstName:&quot;Tom&quot;,lastName:&quot;Hanks&quot;,sayHi: ():string =&gt;&#123;return &quot;Hi there&quot;&#125;&#125;console.log(&quot;Customer Object &quot;)console.log(customer.firstName)console.log(customer.lastName)console.log(customer.sayHi())var employee:IPerson = &#123;firstName:&quot;Jim&quot;,lastName:&quot;Blakes&quot;,sayHi: ():string =&gt;&#123;return &quot;Hello!!!&quot;&#125;&#125;console.log(&quot;Employee Object &quot;)console.log(employee.firstName) console.log(employee.lastName) Interfaces and InheritanceSyntax: Single Interface Inheritance 1Child_interface_name extends super_interface_name Syntax: Multiple Interface Inheritance12Child_interface_name extends super_interface1_name,super_interface2_name,…,super_interfaceN_name Example: Simple Interface Inheritance 123456789101112interface Person &#123;age:number&#125;interface Musician extends Person &#123;instrument:string&#125;var drummer = &lt;Musician&gt;&#123;&#125;;drummer.age = 27drummer.instrument = "Drums"console.log("Age: "+drummer.age) console.log("Instrument: "+drummer.instrument) On compiling, it will generate following JavaScript code. 123456//Generated by typescript 1.8.10var drummer = &#123;&#125;;drummer.age = 27;drummer.instrument = "Drums";console.log("Age: " + drummer.age);console.log("Instrument: " + drummer.instrument); ClassesTypescript gives built in support for this concept called class. JavaScript ES5 or earlier didn’t support classes. Typescript gets this feature from ES6. Creating classesSyntax123class class_name &#123;//class scope&#125; A class definition can include the following Fields − A field is any variable declared in a class. Fields represent data pertaining to objects Constructors − Responsible for allocating memory for the objects of the class Functions − Functions represent actions an object can take. They are also at times referred to as methods Example: Declaring a class1234567891011121314class Car &#123;//fieldengine:string;//constructorconstructor(engine:string) &#123;this.engine = engine&#125;//functiondisp():void &#123;console.log(&quot;Engine is : &quot;+this.engine)&#125;&#125; Enhanced IDE supportThe development experience with TypeScript is a great improvement over JavaScript. The IDE is informed in real-time by the TypeScript compiler on its rich type information. This gives a couple of major advantages. For example, with TypeScript you can safely do refactorings like renames across your entire codebase. Through code completion you can get inline help on whatever functions a library might offer. No more need to remember them or look them up in online references. Compilation errors are reported directly in the IDE with a red squiggly line while you are busy coding. All in all this allows for a significant gain in productivity compared to working with JavaScript. One can spend more time coding and less time debugging. There is a wide range of IDEs that have excellent support for TypeScript, like Visual Studio &amp; VS code, Atom, Sublime, and IntelliJ/WebStorm. Strict null checksRuntime errors of the form cannot read property ‘x’ of undefined or undefined is not a function are very commonly caused by bugs in JavaScript code. Out of the box TypeScript already reduces the probability of these kinds of errors occurring, since one cannot use a variable that is not known to the TypeScript compiler (with the exception of properties of any typed variables). It is still possible though to mistakenly utilize a variable that is set to undefined. However, with the 2.0 version of TypeScript you can eliminate these kinds of errors all together through the usage of non-nullable types. This works as follows: With strict null checks enabled (--strictNullChecks compiler flag) the TypeScript compiler will not allow undefined to be assigned to a variable unless you explicitly declare it to be of nullable type. For example, let x : number = undefined will result in a compile error. This fits perfectly with type theory, since undefined is not a number. One can define x to be a sum type of number and undefined to correct this: let x : number | undefined = undefined. CompilationTo use TypeScript you need a build process to compile to JavaScript code. The build process generally takes only a couple of seconds depending of course on the size of your project. The TypeScript compiler supports incremental compilation (--watch compiler flag), so that all subsequent changes can be compiled at greater speed. The TypeScript compiler can inline source map information in the generated .js files or create separate .map files. Source map information can be used by debugging utilities like the Chrome DevTools and other IDE’s to relate the lines in the JavaScript to the ones that generated them in the TypeScript. This makes it possible for you to set breakpoints and inspect variables during runtime directly on your TypeScript code. Source map information works pretty good, it was around long before TypeScript, but debugging TypeScript is generally not as great as when using JavaScript directly. Converting from JavaScript to TypeScriptAny .js file can be renamed to a .ts and ran through the TypeScript compiler to get syntactically the same JavaScript code as an output (if it was syntactically correct in the first place). Even when the TypeScript compiler gets compilation errors it will still produce a .js file. It can even accept .js files as input with the --allowJs flag. This allows you to start with TypeScript right away. Unfortunately compilation errors are likely to occur in the beginning. One does need to remember that these are not show-stopping errors like you may be used to with other compilers. Installing TypeScriptThere are two main ways to get the TypeScript tools: Via npm (the Node.js package manager) By installing TypeScript’s Visual Studio plugins For NPM users: npm install -g typescript Building your first TypeScript fileIn your editor, type the following JavaScript code in greeter.ts: 1234567function greeter(person) &#123;return "Hello, " + person;&#125;let user = "Jane User";document.body.innerHTML = greeter(user); Compiling your codeWe used a .ts extension, but this code is just JavaScript. You could have copy/pasted this straight out of an existing JavaScript app. At the command line, run the TypeScript compiler: tsc greeter.ts The result will be a file greeter.js which contains the same JavaScript that you fed in. We’re up and running using TypeScript in our JavaScript app! ResourcesTypeScript Playground ReferencesTypeScript TutorialTypeScript Handbook]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>TypeScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Front-end Handbook JavaScript]]></title>
    <url>%2F2018%2F03%2F01%2FFront-end-Handbook-JavaScript%2F</url>
    <content type="text"><![CDATA[Explain how ‘this’ works in JavaScriptThere’s no simple explanation for this; it is one of the most confusing concepts in JavaScript. A hand-wavey explanation is that the value of this depends on how the function is called. I have read many explanations on this online, and I found Arnav Aggrawal’s explanation to be the clearest. The following rules are applied: If the new keyword is used when calling the function, this inside the function is a brand new object. If apply, call, or bind are used to call/create a function, this inside the function is the object that is passed in as the argument. If a function is called as a method, such as obj.method() — this is the object that the function is a property of. If a function is invoked as a free function invocation, meaning it was invoked without any of the conditions present above, this is the global object. In a browser, it is the window object. If in strict mode (‘use strict’), this will be undefined instead of the global object. If multiple of the above rules apply, the rule that is higher wins and will set the this value. If the function is an ES2015 arrow function, it ignores all the rules above and receives the this value of its surrounding scope at the time it is created. Referenceshttps://codeburst.io/the-simple-rules-to-this-in-javascript-35d97f31bde3https://stackoverflow.com/a/3127440/1751946 What’s the difference between an “attribute” and a “property”?Attributes are defined on the HTML markup but properties are defined on the DOM. To illustrate the difference, imagine we have this text field in our HTML: &lt; input type=”text” value=”Hello”&gt;.123const input = document.querySelector('input');console.log(input.getAttribute('value')); // Helloconsole.log(input.value); // Hello But after you change the value of the text field by adding “World!” to it, this becomes:12console.log(input.getAttribute('value')); // Helloconsole.log(input.value); // Hello World! Referenceshttps://stackoverflow.com/questions/6003819/properties-and-attributes-in-html What is the difference between == and ===?== is the abstract equality operator while === is the strict equality operator. The == operator will compare for equality after doing any necessary type conversions. The === operator will not do type conversion, so if two values are not the same type === will simply return false. When using ==, funky things can happen, such as: 1234561 == '1'; // true1 == [1]; // true1 == true; // true0 == ''; // true0 == '0'; // true0 == false; // true My advice is never to use the == operator, except for convenience when comparing against null or undefined, where a == null will return true if a is null or undefined. 123var a = null;console.log(a == null); // trueconsole.log(a == undefined); // true Referenceshttps://stackoverflow.com/questions/359494/which-equals-operator-vs-should-be-used-in-javascript-comparisons What is “use strict”;? What are the advantages and disadvantages to using it?‘use strict’ is a statement used to enable strict mode to entire scripts or individual functions. Strict mode is a way to opt in to a restricted variant of JavaScript. Advantages: Makes it impossible to accidentally create global variables.Makes assignments which would otherwise silently fail to throw an exception.Makes attempts to delete undeletable properties throw (where before the attempt would simply have no effect).Requires that function parameter names be unique.this is undefined in the global context.It catches some common coding bloopers, throwing exceptions.It disables features that are confusing or poorly thought out. Disadvantages: Many missing features that some developers might be used to.No more access to function.caller and function.arguments.Concatenation of scripts written in different strict modes might cause issues.Overall, I think the benefits outweigh the disadvantages, and I never had to rely on the features that strict mode blocks. I would recommend using strict mode. Referenceshttp://2ality.com/2011/10/strict-mode-hatred.htmlhttp://lucybain.com/blog/2014/js-use-strict/ Explain what a single page app is and how to make one SEO-friendly.Web developers these days refer to the products they build as web apps, rather than websites. While there is no strict difference between the two terms, web apps tend to be highly interactive and dynamic, allowing the user to perform actions and receive a response for their action. Traditionally, the browser receives HTML from the server and renders it. When the user navigates to another URL, a full-page refresh is required and the server sends fresh new HTML for the new page. This is called server-side rendering. However in modern SPAs, client-side rendering is used instead. The browser loads the initial page from the server, along with the scripts (frameworks, libraries, app code) and stylesheets required for the whole app. When the user navigates to other pages, a page refresh is not triggered. The URL of the page is updated via the HTML5 History API. New data required for the new page, usually in JSON format, is retrieved by the browser via AJAX requests to the server. The SPA then dynamically updates the page with the data via JavaScript, which it has already downloaded in the initial page load. This model is similar to how native mobile apps work. The benefits: The app feels more responsive and users do not see the flash between page navigations due to full-page refreshes.Fewer HTTP requests are made to the server, as the same assets do not have to be downloaded again for each page load.Clear separation of the concerns between the client and the server; you can easily build new clients for different platforms (e.g. mobile, chatbots, smart watches) without having to modify the server code. You can also modify the technology stack on the client and server independently, as long as the API contract is not broken. The downsides: Heavier initial page load due to loading of framework, app code, and assets required for multiple pages.There’s an additional step to be done on your server which is to configure it to route all requests to a single entry point and allow client-side routing to take over from there.SPAs are reliant on JavaScript to render content, but not all search engines execute JavaScript during crawling, and they may see empty content on your page. This inadvertently hurts the Search Engine Optimization (SEO) of your app. However, most of the time, when you are building apps, SEO is not the most important factor, as not all the content needs to be indexable by search engines. To overcome this, you can either server-side render your app or use services such as Prerender to “render your javascript in a browser, save the static HTML, and return that to the crawlers”. Referenceshttps://github.com/grab/front-end-guide#single-page-apps-spashttp://stackoverflow.com/questions/21862054/single-page-app-advantages-and-disadvantageshttp://blog.isquaredsoftware.com/presentations/2016-10-revolution-of-web-dev/https://medium.freecodecamp.com/heres-why-client-side-rendering-won-46a349fadb52 What are the pros and cons of using Promises instead of callbacks?Pros Avoid callback hell which can be unreadable.Makes it easy to write sequential asynchronous code that is readable with .then().Makes it easy to write parallel asynchronous code with Promise.all(). Cons Slightly more complex code (debatable).In older browsers where ES2015 is not supported, you need to load a polyfill in order to use it. Explain the difference between synchronous and asynchronous functions.Synchronous functions are blocking while asynchronous functions are not. In synchronous functions, statements complete before the next statement is run. In this case the program is evaluated exactly in order of the statements and execution of the program is paused if one of the statements take a very long time. Asynchronous functions usually accept a callback as a parameter and execution continues on the next line immediately after the asynchronous function is invoked. The callback is only invoked when the asynchronous operation is complete and the call stack is empty. Heavy duty operations such as loading data from a web server or querying a database should be done asynchronously so that the main thread can continue executing other operations instead of blocking until that long operation to complete (in the case of browsers, the UI will freeze). Explain the differences on the usage of foo between function foo() {} and var foo = function() {}The former is a function declaration while the latter is a function expression. The key difference is that function declarations have its body hoisted but the bodies of function expressions are not (they have the same hoisting behaviour as variables). For more explanation on hoisting, refer to the question above on hoisting. If you try to invoke a function expression before it is defined, you will get an Uncaught TypeError: XXX is not a function error. Function Declaration1234foo(); // 'FOOOOO'function foo() &#123;console.log('FOOOOO');&#125; Function Expression1234foo(); // Uncaught TypeError: foo is not a functionvar foo = function() &#123;console.log(&apos;FOOOOO&apos;);&#125;; Referenceshttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function What are the differences between null and undefined?undefined undefined means, value of the variable is not defined. JavaScript has a global variable undefined whose value is “undefined” and typeof undefined is also “undefined”. Remember, undefined is not a constant or a keyword. undefined is a type with exactly one value: undefined. Assigning a new value to it does not change the value of the type undefined. 8 Ways to get Undefined: A declared variable without assigning any value to it. Implicit returns of functions due to missing return statements. return statements that do not explicitly return anything. Lookups of non-existent properties in an object. Function parameters that have not passed. Anything that has been set to the value of undefined. Any expression in the form of void(expression) The value of the global variable undefined null null means empty or non-existent value which is used by programmers to indicate “no value”. null is a primitive value and you can assign null to any variable. null is not an object, it is a primitive value. For example, you cannot add properties to it. Sometimes people wrongly assume that it is an object, because typeof null returns “object”. Btw, null == undefined. Does JavaScript pass parameter by value or by reference?Primitive type (string, number, etc.) are passed by value and objects are passed by reference. If you change a property of the passed object, the change will be affected. However, you assign a new object to the passed object, the changes will not be reflected. 123456789101112131415161718192021222324var num = 10,name = "Addy Osmani",obj1 = &#123;value: "first value"&#125;,obj2 = &#123;value: "second value"&#125;,obj3 = obj2;function change(num, name, obj1, obj2) &#123;num = num * 10;name = "Paul Irish";obj1 = obj2;obj2.value = "new value";&#125;change(num, name, obj1, obj2);console.log(num); // 10console.log(name);// "Addy Osmani"console.log(obj1.value);//"first value"console.log(obj2.valuee);//"new value"console.log(obj3.valuee);//"new value"]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Tutorial(2) -- Define a container with Dockerfile]]></title>
    <url>%2F2018%2F02%2F16%2FDocker-Tutorial-Define-a-container-with-Dockerfile%2F</url>
    <content type="text"><![CDATA[Define a container with DockerfileCreate an empty directory. Change directories (cd) into the new directory, create a file called Dockerfile, copy-and-paste the following content into that file, and save it. Take note of the comments that explain each statement in your new Dockerfile. 1234567891011121314151617181920# Use an official Python runtime as a parent imageFROM python:2.7-slim# Set the working directory to /appWORKDIR /app# Copy the current directory contents into the container at /appADD . /app# Install any needed packages specified in requirements.txtRUN pip install --trusted-host pypi.python.org -r requirements.txt# Make port 80 available to the world outside this containerEXPOSE 80# Define environment variableENV NAME World# Run app.py when the container launchesCMD [&quot;python&quot;, &quot;app.py&quot;] This Dockerfile refers to a couple of files we haven’t created yet, namely app.py and requirements.txt. Let’s create those next. The app itselfCreate two more files, requirements.txt and app.py, and put them in the same folder with the Dockerfile. This completes our app, which as you can see is quite simple. When the above Dockerfile is built into an image, app.py and requirements.txt is present because of that Dockerfile’s ADD command, and the output from app.py is accessible over HTTP thanks to the EXPOSE command. requirements.txt FlaskRedis app.py123456789101112131415161718192021222324from flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# Connect to Redisredis = Redis(host=&quot;redis&quot;, db=0, socket_connect_timeout=2, socket_timeout=2)app = Flask(__name__)@app.route(&quot;/&quot;)def hello():try:visits = redis.incr(&quot;counter&quot;)except RedisError:visits = &quot;&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;&quot;html = &quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot; \&quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot; \&quot;&lt;b&gt;Visits:&lt;/b&gt; &#123;visits&#125;&quot;return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname(), visits=visits)if __name__ == &quot;__main__&quot;:app.run(host=&apos;0.0.0.0&apos;, port=80) Now we see that pip install -r requirements.txt installs the Flask and Redis libraries for Python, and the app prints the environment variable NAME, as well as the output of a call to socket.gethostname(). Finally, because Redis isn’t running (as we’ve only installed the Python library, and not Redis itself), we should expect that the attempt to use it here fails and produces the error message. That’s it! You don’t need Python or anything in requirements.txt on your system, nor does building or running this image install them on your system. It doesn’t seem like you’ve really set up an environment with Python and Flask, but you have. Build the appWe are ready to build the app. Make sure you are still at the top level of your new directory. Here’s what ls should show: $ lsDockerfile app.py requirements.txt Now run the build command. This creates a Docker image, which we’re going to tag using -t so it has a friendly name. docker build -t friendlyhello . Where is your built image? It’s in your machine’s local Docker image registry: $ docker image ls REPOSITORY TAG IMAGE IDfriendlyhello latest 326387cea398 Run the appRun the app, mapping your machine’s port 4000 to the container’s published port 80 using -p: docker run -p 4000:80 friendlyhello You should see a message that Python is serving your app at http://0.0.0.0:80. But that message is coming from inside the container, which doesn’t know you mapped port 80 of that container to 4000, making the correct URL http://localhost:4000. Go to that URL in a web browser to see the display content served up on a web page. Note: If you are using Docker Toolbox on Windows 7, use the Docker Machine IP instead of localhost. For example, http://192.168.99.100:4000/. To find the IP address, use the command docker-machine ip. You can also use the curl command in a shell to view the same content. 123$ curl http://localhost:4000&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 8fc990912a14&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt; Now let’s run the app in the background, in detached mode: docker run -d -p 4000:80 friendlyhello You get the long container ID for your app and then are kicked back to your terminal. Your container is running in the background. You can also see the abbreviated container ID with docker container ls (and both work interchangeably when running commands): $ docker container ls CONTAINER ID IMAGE COMMAND CREATED 1fa4ab2cf395 friendlyhello “python app.py” 28 seconds ago Notice that CONTAINER ID matches what’s on http://localhost:4000. Now use docker container stop to end the process, using the CONTAINER ID, like so: docker container stop 1fa4ab2cf395 Share your imageA registry is a collection of repositories, and a repository is a collection of images—sort of like a GitHub repository, except the code is already built. An account on a registry can create many repositories. The docker CLI uses Docker’s public registry by default. Note: We use Docker’s public registry here just because it’s free and pre-configured, but there are many public ones to choose from, and you can even set up your own private registry using Docker Trusted Registry. Log in with your Docker IDIf you don’t have a Docker account, sign up for one at cloud.docker.com. Make note of your username. Log in to the Docker public registry on your local machine. $ docker login Tag the imageThe notation for associating a local image with a repository on a registry is username/repository:tag. The tag is optional, but recommended, since it is the mechanism that registries use to give Docker images a version. Give the repository and tag meaningful names for the context, such as get-started:part2. This puts the image in the get-started repository and tag it as part2. Now, put it all together to tag the image. Run docker tag image with your username, repository, and tag names so that the image uploads to your desired destination. The syntax of the command is: docker tag image username/repository:tag For example: docker tag friendlyhello john/get-started:part2 Publish the imageUpload your tagged image to the repository: docker push username/repository:tag Once complete, the results of this upload are publicly available. If you log in to Docker Hub, you see the new image there, with its pull command. Pull and run the image from the remote repositoryFrom now on, you can use docker run and run your app on any machine with this command: docker run -p 4000:80 username/repository:tag If the image isn’t available locally on the machine, Docker pulls it from the repository. $ docker run -p 4000:80 john/get-started:part2 Unable to find image ‘john/get-started:part2’ locallypart2: Pulling from john/get-started10a267c67f42: Already existsf68a39a6a5e4: Already exists9beaffc0cf19: Already exists3c1fe835fb6b: Already exists4c9f1fa8fcb8: Already existsee7d8f576a14: Already existsfbccdcced46e: Already existsDigest: sha256:0601c866aab2adcc6498200efd0f754037e909e5fd42069adeff72d1e2439068Status: Downloaded newer image for john/get-started:part2 Running on http://0.0.0.0:80/ (Press CTRL+C to quit) No matter where docker run executes, it pulls your image, along with Python and all the dependencies from requirements.txt, and runs your code. It all travels together in a neat little package, and you don’t need to install anything on the host machine for Docker to run it. Here is a list of the basic Docker commands from this page, and some related ones if you’d like to explore a bit before moving on. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Create image using this directory&apos;s Dockerfiledocker build -t friendlyhello .# Run &quot;friendlyname&quot; mapping port 4000 to 80docker run -p 4000:80 friendlyhello# Same thing, but in detached modedocker run -d -p 4000:80 friendlyhello# List all running containersdocker container ls# List all containers, even those not runningdocker container ls -a# Gracefully stop the specified containerdocker container stop &lt;hash&gt;# Force shutdown of the specified containerdocker container kill &lt;hash&gt;# Remove specified container from this machinedocker container rm &lt;hash&gt;# Remove all containersdocker container rm \$(docker container ls -a -q)# List all images on this machinedocker image ls -a# Remove specified image from this machinedocker image rm &lt;image id&gt;# Remove all images from this machinedocker image rm $(docker image ls -a -q)# Log in this CLI session using your Docker credentialsdocker login# Tag &lt;image&gt; for upload to registrydocker tag &lt;image&gt; username/repository:tag# Upload tagged image to registrydocker push username/repository:tag# Run image from a regdocker run username/repository:tag ReferencesA Docker Tutorial for BeginnersDocker TutorialDockerfile referenceDocs Docker - Get Started]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Tutorial (1) -- Introduction]]></title>
    <url>%2F2018%2F02%2F15%2FDocker-Tutorial%2F</url>
    <content type="text"><![CDATA[What is Docker?Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Why Docker?Environment is essential for running software and it’s inconvenient to set up the same environment on different devices. Virtual machine could solve the above problem by creating an isolated operating system as the environment. However, virtual machine has disadvantages, such as consuming resources, creating redundant steps and slow initialization. While Docker utilizes the virtualization technology in the Linux kernel, it does not create virtual machines. Until a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows. Once you are done installing Docker, test your Docker installation by running the following: 12345678910111213141516171819##List Docker CLI commandsdockerdocker container --help## Display Docker version and infodocker --versiondocker versiondocker info## Excecute Docker imagedocker run hello-world## List Docker imagesdocker image ls## List Docker containers (running, all, all in quiet mode)docker container lsdocker container ls -alldocker container ls -a -q Docker Example(Busybox)To get started, let’s run the following in our terminal: $ docker pull busybox The pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system. $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE busybox latest c51f86c28340 4 weeks ago 1.109 MB Docker RunLet’s now run a Docker container based on this image. $ docker run busybox The docker ps command shows you all containers that are currently running. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Before we move ahead though, let’s quickly talk about deleting containers. To do that, you can run the docker rm command. $ docker rm 305297d7a235 ff0a5c3750b9 On deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run: \$ docker rm $(docker ps -a -q -f status=exited) This command deletes all containers that have a status of exited. In case you’re wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that’ll be useful is the –rm flag that can be passed to docker run which automatically deletes the container once it’s exited from. For one off docker runs, –rm flag is very useful. Lastly, you can also delete images that you no longer need by running docker rmi. TerminologyIn the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem. Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command. Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users. Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system to which clients talk to. Dockerfile - A Dockerfile is a text document that contains all the commands you would normally execute manually in order to build a Docker image. Docker can build images automatically by reading the instructions from a Dockerfile. Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images. Images - Docker images are the basis of containers. An Image is an ordered collection of root filesystem changes and the corresponding execution parameters for use within a container runtime. An image typically contains a union of layered filesystems stacked on top of each other. An image does not have state and it never changes. Node - A node is a physical or virtual machine running an instance of the Docker Engine in swarm mode. Manager nodes perform swarm management and - orchestration duties. By default manager nodes are also worker nodes. Worker nodes execute tasks. Registry - A Registry is a hosted service containing repositories of images which responds to the Registry API. The default registry can be accessed using a browser at Docker Hub or using the docker search command. Repository - A repository is a set of Docker images. A repository can be shared by pushing it to a registry server. The different images in the repository can be labeled using tags. Service - A service is the definition of how you want to run your application containers in a swarm. At the most basic level a service defines which container image to run in the swarm and which commands to run in the container. For orchestration purposes, the service defines the “desired state”, meaning how many containers to run as tasks and constraints for deploying the containers. Stack - A stack is a group of interrelated services that share dependencies, and can be orchestrated and scaled together. A single stack is capable of defining and coordinating the functionality of an entire application (though very complex applications may want to use multiple stacks). Swarm - A swarm is a cluster of one or more Docker Engines running in swarm mode. Tag - A tag is a label applied to a Docker image in a repository. Tags are how various images in a repository are distinguished from each other. ReferencesA Docker Tutorial for BeginnersDocker TutorialDockerfile referenceDocs Docker - Get Started]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DevOps Introduction]]></title>
    <url>%2F2018%2F02%2F15%2FDevOps-Introduction%2F</url>
    <content type="text"><![CDATA[DefinitionDevOps is a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality. DevOps toolchain Code — code development and review, source code management tools, code merging Build — continuous integration tools, build status Test — continuous testing tools that provide feedback on business risks Package — artifact repository, application pre-deployment staging Release — change management, release approvals, release automation Configure — infrastructure configuration and management, Infrastructure as Code tools Monitor — applications performance monitoring, end–user experience Why DevOpsThe Water-fall model worked fine and served well for many years however it had some challenges. In the following diagram the challenges of Waterfall Model are highlighted. In the above diagram you can see that both Development and Operations had challenges in the Waterfall Model. From Developers point of view there were majorly two challenges: After Development, the code deployment time was huge. Pressure of work on old, pending and new code was high because development and deployment time was high. On the other hand, Operations was also not completely satisfied. There were four major challenges they faced as per the above diagram: It was difficult to maintain ~100% uptime of the production environment. Infrastructure Automation tools were not very affective. Number of severs to be monitored keeps on increasing with time and hence the complexity. It was very difficult to provide feedback and diagnose issue in the product. In the following diagram proposed solution to the challenges of Waterfall Model are highlighted. In the above diagram, Probable Solutions for the issues faced by Developers and Operations are highlighted in blue. This sets the guidelines for an Ideal Software Development strategy. From Developers point of view: A system which enables code deployment without any delay or wait time. A system where work happens on the current code itself i.e. development sprints are short and well planned. From Operations point of view: System should have at-least 99% uptime. Tools &amp; systems are there in place for easy administration. Effective monitoring and feedbacks system should be there. Better Collaboration between Development &amp; Operations and is common requirement for Developers and Operations team. ReferencesDevOps - WikipediaDevOps Tutorial : Introduction To DevOpsWhat Is DevOps?]]></content>
      <categories>
        <category>Software Development</category>
      </categories>
      <tags>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrum Introduction]]></title>
    <url>%2F2018%2F02%2F14%2FScrum-Introduction%2F</url>
    <content type="text"><![CDATA[Definition of ScrumScrum (n): A framework within which people can address complex adaptive problems, while productively and creatively delivering products of the highest possible value. Scrum is:• Lightweight• Simple to understand• Difficult to master Uses of Scrum Research and identify viable markets, technologies, and product capabilities; Develop products and enhancements; Release products and enhancements, as frequently as many times per day; Develop and sustain Cloud (online, secure, on-demand) and other operationalenvironments for product use; and, Sustain and renew products. Scrum TheoryScrum is founded on empirical process control theory, or empiricism. Empiricism asserts that knowledge comes from experience and making decisions based on what is known. Scrum employs an iterative, incremental approach to optimize predictability and control risk. Three pillars uphold every implementation of empirical process control:transparency, inspection, and adaptation. TransparencySignificant aspects of the process must be visible to those responsible for the outcome. Transparency requires those aspects be defined by a common standard so observers share a common understanding of what is being seen. For example: • A common language referring to the process must be shared by all participants; and,• Those performing the work and those inspecting the resulting increment must share a common definition of “Done”. InspectionScrum users must frequently inspect Scrum artifacts and progress toward a Sprint Goal to detect undesirable variances. Their inspection should not be so frequent that inspection gets in the way of the work. Inspections are most beneficial when diligently performed by skilled inspectors at the point of work. AdaptationIf an inspector determines that one or more aspects of a process deviate outside acceptable limits, and that the resulting product will be unacceptable, the process or the material being processed must be adjusted. An adjustment must be made as soon as possible to minimize further deviation. Scrum prescribes four formal events for inspection and adaptation, as described in the Scrum Events section of this document: • Sprint Planning• Daily Scrum• Sprint Review• Sprint Retrospective The Scrum TeamThe Scrum Team consists of a Product Owner, the Development Team, and a Scrum Master. Scrum Teams are self-organizing and cross-functional. Self-organizing teams choose how best to accomplish their work, rather than being directed by others outside the team. Cross-functional teams have all competencies needed to accomplish the work without depending on others not part of the team. The team model in Scrum is designed to optimize flexibility, creativity, and productivity. The Product OwnerThe Product Owner is responsible for maximizing the value of the product resulting from work of the Development Team. How this is done may vary widely across organizations, Scrum Teams, and individuals. The Product Owner is the sole person responsible for managing the Product Backlog. Product Backlog management includes: • Clearly expressing Product Backlog items;• Ordering the items in the Product Backlog to best achieve goals and missions;• Optimizing the value of the work the Development Team performs;• Ensuring that the Product Backlog is visible, transparent, and clear to all, and shows what the Scrum Team will work on next;• Ensuring the Development Team understands items in the Product Backlog to the level needed. The Product Owner may do the above work, or have the Development Team do it. However, the Product Owner remains accountable. The Product Owner is one person, not a committee. The Product Owner may represent the desires of a committee in the Product Backlog, but those wanting to change a Product Backlog item’s priority must address the Product Owner. For the Product Owner to succeed, the entire organization must respect his or her decisions. The Product Owner’s decisions are visible in the content and ordering of the Product Backlog. No one can force the Development Team to work from a different set of requirements. The Development TeamThe Development Team consists of professionals who do the work of delivering a potentially releasable Increment of “Done” product at the end of each Sprint. A “Done” increment is required at the Sprint Review. Only members of the Development Team create the Increment. Development Teams are structured and empowered by the organization to organize and manage their own work. The resulting synergy optimizes the Development Team’s overall efficiency and effectiveness. Development Teams have the following characteristics: • They are self-organizing. No one (not even the Scrum Master) tells the Development Team how to turn Product Backlog into Increments of potentially releasable functionality;• Development Teams are cross-functional, with all the skills as a team necessary to create a product Increment;• Scrum recognizes no titles for Development Team members, regardless of the work being performed by the person;• Scrum recognizes no sub-teams in the Development Team, regardless of domains that need to be addressed like testing, architecture, operations, or business analysis;• Individual Development Team members may have specialized skills and areas of focus, but accountability belongs to the Development Team as a whole. Development Team SizeOptimal Development Team size is small enough to remain nimble and large enough to complete significant work within a Sprint. Fewer than three Development Team members decrease interaction and results in smaller productivity gains. Smaller Development Teams may encounter skill constraints during the Sprint, causing the Development Team to be unable to deliver a potentially releasable Increment. Having more than nine members requires too much coordination. Large Development Teams generate too much complexity for an empirical process to be useful. The Product Owner and Scrum Master roles are not included in this count unless they are also executing the work of the Sprint Backlog. The Scrum MasterThe Scrum Master is responsible for promoting and supporting Scrum as defined in the Scrum Guide. Scrum Masters do this by helping everyone understand Scrum theory, practices, rules, and values. The Scrum Master is a servant-leader for the Scrum Team. The Scrum Master helps those outside the Scrum Team understand which of their interactions with the Scrum Team are helpful and which aren’t. The Scrum Master helps everyone change these interactions to maximize the value created by the Scrum Team. Scrum Master Service to the Product OwnerThe Scrum Master serves the Product Owner in several ways, including:• Ensuring that goals, scope, and product domain are understood by everyone on the ScrumTeam as well as possible;• Finding techniques for effective Product Backlog management;• Helping the Scrum Team understand the need for clear and concise Product Backlog items;• Understanding product planning in an empirical environment;• Ensuring the Product Owner knows how to arrange the Product Backlog to maximize value;• Understanding and practicing agility;• Facilitating Scrum events as requested or needed. Scrum Master Service to the Development TeamThe Scrum Master serves the Development Team in several ways, including:• Coaching the Development Team in self-organization and cross-functionality;• Helping the Development Team to create high-value products;• Removing impediments to the Development Team’s progress;• Facilitating Scrum events as requested or needed;• Coaching the Development Team in organizational environments in which Scrum is not yet fully adopted and understood. Scrum Master Service to the OrganizationThe Scrum Master serves the organization in several ways, including:• Leading and coaching the organization in its Scrum adoption;• Planning Scrum implementations within the organization;• Helping employees and stakeholders understand and enact Scrum and empirical productdevelopment;• Causing change that increases the productivity of the Scrum Team;• Working with other Scrum Masters to increase the effectiveness of the application of Scrum in the organization. Scrum EventsPrescribed events are used in Scrum to create regularity and to minimize the need for meetings not defined in Scrum. All events are time-boxed events, such that every event has a maximum duration. Once a Sprint begins, its duration is fixed and cannot be shortened or lengthened. The remaining events may end whenever the purpose of the event is achieved, ensuring an appropriate amount of time is spent without allowing waste in the process. The Scrum Events are: Sprint Sprint Planning Daily Scrum Sprint Review Sprint Retrospective The SprintThe heart of Scrum is a Sprint, a time-box of one month or less during which a “Done”, useable, and potentially releasable product Increment is created. Sprints have consistent durations throughout a development effort. A new Sprint starts immediately after the conclusion of theprevious Sprint. During the Sprint:• No changes are made that would endanger the Sprint Goal;• Quality goals do not decrease;• Scope may be clarified and re-negotiated between the Product Owner and Development Team as more is learned. Each Sprint may be considered a project with no more than a one-month horizon. Like projects, Sprints are used to accomplish something. Each Sprint has a goal of what is to be built, a design and flexible plan that will guide building it, the work, and the resultant product increment. Sprints are limited to one calendar month. When a Sprint’s horizon is too long the definition of what is being built may change, complexity may rise, and risk may increase. Sprints enable predictability by ensuring inspection and adaptation of progress toward a Sprint Goal at least every calendar month. Sprints also limit risk to one calendar month of cost. Sprint PlanningThe work to be performed in the Sprint is planned at the Sprint Planning. This plan is created by the collaborative work of the entire Scrum Team. Sprint Planning is time-boxed to a maximum of eight hours for a one-month Sprint. For shorter Sprints, the event is usually shorter. The Scrum Master ensures that the event takes place and that attendants understand its purpose. The Scrum Master teaches the Scrum Team to keep it within the time-box. Sprint Planning answers the following:• What can be delivered in the Increment resulting from the upcoming Sprint?• How will the work needed to deliver the Increment be achieved? Topic One: What can be done this Sprint?The Development Team works to forecast the functionality that will be developed during the Sprint. The Product Owner discusses the objective that the Sprint should achieve and the Product Backlog items that, if completed in the Sprint, would achieve the Sprint Goal. The entire Scrum Team collaborates on understanding the work of the Sprint. Topic Two: How will the chosen work get done?Having set the Sprint Goal and selected the Product Backlog items for the Sprint, the Development Team decides how it will build this functionality into a “Done” product Increment during the Sprint. The Product Backlog items selected for this Sprint plus the plan for delivering them is called the Sprint Backlog. By the end of the Sprint Planning, the Development Team should be able to explain to the Product Owner and Scrum Master how it intends to work as a self-organizing team to accomplish the Sprint Goal and create the anticipated Increment. Sprint GoalThe Sprint Goal is an objective set for the Sprint that can be met through the implementation of Product Backlog. It provides guidance to the Development Team on why it is building the Increment. It is created during the Sprint Planning meeting. The Sprint Goal gives the Development Team some flexibility regarding the functionality implemented within the Sprint. Daily ScrumThe Daily Scrum is a 15-minute time-boxed event for the Development Team. The Daily Scrum is held every day of the Sprint. At it, the Development Team plans work for the next 24 hours. This optimizes team collaboration and performance by inspecting the work since the last Daily Scrum and forecasting upcoming Sprint work. The Daily Scrum is held at the same time and place each day to reduce complexity. Sprint ReviewA Sprint Review is held at the end of the Sprint to inspect the Increment and adapt the Product Backlog if needed. During the Sprint Review, the Scrum Team and stakeholders collaborate about what was done in the Sprint. The Sprint Review includes the following elements:• Attendees include the Scrum Team and key stakeholders invited by the Product Owner;• The Product Owner explains what Product Backlog items have been “Done” and what has not been “Done”;• The Development Team discusses what went well during the Sprint, what problems it ran into, and how those problems were solved;• The Development Team demonstrates the work that it has “Done” and answers questions about the Increment;• The Product Owner discusses the Product Backlog as it stands. He or she projects likely target and delivery dates based on progress to date (if needed);• The entire group collaborates on what to do next, so that the Sprint Review provides valuable input to subsequent Sprint Planning;• Review of how the marketplace or potential use of the product might have changed what is the most valuable thing to do next;• Review of the timeline, budget, potential capabilities, and marketplace for the next anticipated releases of functionality or capability of the product. The result of the Sprint Review is a revised Product Backlog that defines the probable ProductBacklog items for the next Sprint. The Product Backlog may also be adjusted overall to meet newopportunities. Sprint RetrospectiveThe Sprint Retrospective is an opportunity for the Scrum Team to inspect itself and create a plan for improvements to be enacted during the next Sprint. The purpose of the Sprint Retrospective is to:• Inspect how the last Sprint went with regards to people, relationships, process, and tools;• Identify and order the major items that went well and potential improvements;• Create a plan for implementing improvements to the way the Scrum Team does its work. Scrum ArtifactsScrum’s artifacts represent work or value to provide transparency and opportunities for inspection and adaptation. Artifacts defined by Scrum are specifically designed to maximize transparency of key information so that everybody has the same understanding of the artifact. The Scrum Artifacts are: Product Backlog Sprint Backlog Increment Referenceswhat-is-scrum2017-Scrum-Guide]]></content>
      <categories>
        <category>Software Development</category>
      </categories>
      <tags>
        <tag>Scrum</tag>
        <tag>Agile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LINQ Tutorial]]></title>
    <url>%2F2018%2F02%2F14%2FLINQ%2F</url>
    <content type="text"><![CDATA[OverviewLanguage Integrated Query (LINQ, pronounced “link”) is a Microsoft .NET Framework component that adds native data querying capabilities to .NET languages. All LINQ query operations consist of three distinct actions: Obtain the data source. Create the query. Execute the query. The following example shows how the three parts of a query operation are expressed in source code:12345678910111213141516171819202122class IntroToLINQ&#123; static void Main() &#123; // The Three Parts of a LINQ Query: // 1. Data source. int[] numbers = new int[7] &#123; 0, 1, 2, 3, 4, 5, 6 &#125;; // 2. Query creation. // numQuery is an IEnumerable&lt;int&gt; var numQuery = from num in numbers where (num % 2) == 0 select num; // 3. Query execution. foreach (int num in numQuery) &#123; Console.Write("&#123;0,1&#125; ", num); &#125; &#125;&#125; The following illustration shows the complete query operation. In LINQ the execution of the query is distinct from the query itself; in other words you have not retrieved any data just by creating a query variable. Why LINQIn terms of why we should use LINQ, we can explain it by an example: to find a student object in a student array. Before C# 2.0, we could find specific object in array only by using like “foreach” or “for”. For example: 1234567891011121314151617181920212223242526272829303132333435class Student&#123;public int StudentID &#123; get; set; &#125;public String StudentName &#123; get; set; &#125;public int Age &#123; get; set; &#125;&#125;class Program&#123; static void Main(string[] args) &#123; Student[] studentArray = &#123; new Student() &#123; StudentID = 1, StudentName = "John", Age = 18 &#125;, new Student() &#123; StudentID = 2, StudentName = "Steve", Age = 21 &#125;, new Student() &#123; StudentID = 3, StudentName = "Bill", Age = 25 &#125;, new Student() &#123; StudentID = 4, StudentName = "Ram" , Age = 20 &#125;, new Student() &#123; StudentID = 5, StudentName = "Ron" , Age = 31 &#125;, new Student() &#123; StudentID = 6, StudentName = "Chris", Age = 17 &#125;, new Student() &#123; StudentID = 7, StudentName = "Rob",Age = 19 &#125;, &#125;; Student[] students = new Student[10]; int i = 0; foreach (Student std in studentArray) &#123; if (std.Age &gt; 12 &amp;&amp; std.Age &lt; 20) &#123; students[i] = std; i++; &#125; &#125; &#125;&#125; To increase readability and maintainability, in C# 2.0 delegate was imported:123456789101112131415161718192021222324252627282930313233343536373839delegate bool FindStudent(Student std);class StudentExtension&#123; public static Student[] where(Student[] stdArray, FindStudent del) &#123; int i=0; Student[] result = new Student[10]; foreach (Student std in stdArray) if (del(std)) &#123; result[i] = std; i++; &#125; return result; &#125;&#125; class Program &#123; static void Main(string[] args) &#123; Student[] studentArray = &#123; new Student() &#123; StudentID = 1, StudentName = "John", Age = 18 &#125; , new Student() &#123; StudentID = 2, StudentName = "Steve", Age = 21 &#125; , new Student() &#123; StudentID = 3, StudentName = "Bill", Age = 25 &#125; , new Student() &#123; StudentID = 4, StudentName = "Ram" , Age = 20 &#125; , new Student() &#123; StudentID = 5, StudentName = "Ron" , Age = 31 &#125; , new Student() &#123; StudentID = 6, StudentName = "Chris", Age = 17 &#125; , new Student() &#123; StudentID = 7, StudentName = "Rob",Age = 19 &#125; , &#125;; Student[] students = StudentExtension.where(studentArray, delegate(Student std)&#123; return std.Age &gt; 12 &amp;&amp; std.Age &lt; 20; &#125;); &#125; &#125;&#125; Thus, if we want to find students with different names, we can reuse the delegate instead of using different for-loop. However, C# code could be more compact and readable. LINQ was imported in C# 3.0. For example:123456789101112131415161718192021222324class Program&#123; static void Main(string[] args) &#123; Student[] studentArray = &#123; new Student() &#123; StudentID = 1, StudentName = "John", age = 18 &#125; , new Student() &#123; StudentID = 2, StudentName = "Steve", age = 21 &#125; , new Student() &#123; StudentID = 3, StudentName = "Bill", age = 25 &#125; , new Student() &#123; StudentID = 4, StudentName = "Ram" , age = 20 &#125; , new Student() &#123; StudentID = 5, StudentName = "Ron" , age = 31 &#125; , new Student() &#123; StudentID = 6, StudentName = "Chris", age = 17 &#125; , new Student() &#123; StudentID = 7, StudentName = "Rob",age = 19 &#125; , &#125;; // Use LINQ to find teenager students Student[] teenAgerStudents = studentArray.Where(s =&gt; s.age &gt; 12 &amp;&amp; s.age &lt; 20).ToArray(); // Use LINQ to find first student whose name is Bill Student bill = studentArray.Where(s =&gt; s.StudentName == "Bill").FirstOrDefault(); // Use LINQ to find student whose StudentID is 5 Student student5 = studentArray.Where(s =&gt; s.StudentID == 5).FirstOrDefault(); &#125;&#125; Syntax of LINQThere are two syntaxes of LINQ. These are the following ones. Lamda (Method) Syntax12var longWords = words.Where( w ⇒ w.length &gt; 10);Dim longWords = words.Where(Function(w) w.length &gt; 10) Query (Comprehension) Syntax12var longwords = from w in words where w.length &gt; 10;Dim longwords = from w in words where w.length &gt; 10 Types of LINQThe types of LINQ are mentioned below in brief. LINQ to Objects LINQ to XML(XLINQ) LINQ to DataSet LINQ to SQL (DLINQ) LINQ to Entities Walkthrough: Writing Queries in C# (LINQ)This walkthrough demonstrates the C# language features that are used to write LINQ query expressions. Create a C# ProjectTo create a project in Visual Studio Start Visual Studio. On the menu bar, choose File, New, Project.The New Project dialog box opens. Expand Installed, expand Templates, expand Visual C#, and then choose Console Application. In the Name text box, enter a different name or accept the default name, and then choose the OK button.The new project appears in Solution Explorer. Notice that your project has a reference to System.Core.dll and a using directive for the System.Linq namespace. Create an in-Memory Data Source To add the data sourceAdd the Student class and the initialized list of students to the Program class in your project. 123456789101112131415public class Student&#123; public string First &#123; get; set; &#125; public string Last &#123; get; set; &#125; public int ID &#123; get; set; &#125; public List&lt;int&gt; Scores;&#125;// Create a data source by using a collection initializer.static List&lt;Student&gt; students = new List&lt;Student&gt;&#123; new Student &#123;First="Svetlana", Last="Omelchenko", ID=111, Scores= new List&lt;int&gt; &#123;97, 92, 81, 60&#125;&#125;, new Student &#123;First="Claire", Last="O'Donnell", ID=112, Scores= new List&lt;int&gt; &#123;75, 84, 91, 39&#125;&#125;, new Student &#123;First="Sven", Last="Mortensen", ID=113, Scores= new List&lt;int&gt; &#123;88, 94, 65, 91&#125;&#125;,&#125;; To add a new Student to the Students listAdd a new Student to the Students list and use a name and test scores of your choice. Try typing all the new student information in order to better learn the syntax for the object initializer. Create the Query To create a simple queryIn the application’s Main method, create a simple query that, when it is executed, will produce a list of all students whose score on the first test was greater than 90.123456// Create the query.// The first line could also be written as "var studentQuery ="IEnumerable&lt;Student&gt; studentQuery = from student in students where student.Scores[0] &gt; 90 select student; Execute the Query To execute the query Now write the foreach loop that will cause the query to execute. Note the following about the code: Each element in the returned sequence is accessed through the iteration variable in the foreach loop. The type of this variable is Student, and the type of the query variable is compatible, IEnumerable. After you have added this code, build and run the application to see the results in the Console window. 123456// Execute the query.// var could be used here also.foreach (Student student in studentQuery)&#123; Console.WriteLine("&#123;0&#125;, &#123;1&#125;", student.Last, student.First);&#125; To add another filter conditionYou can combine multiple Boolean conditions in the where clause in order to further refine a query. 1where student.Scores[0] &gt; 90 &amp;&amp; student.Scores[3] &lt; 80 Modify the QueryTo order the resultsYou can order the returned sequence by any accessible field in the source elements.1orderby student.Last ascending To group the results Grouping is a powerful capability in query expressions. A query with a group clause produces a sequence of groups, and each group itself contains a Key and a sequence that consists of all the members of that group.The following new query groups the students by using the first letter of their last name as the key. 1234// studentQuery2 is an IEnumerable&lt;IGrouping&lt;char, Student&gt;&gt;var studentQuery2 = from student in students group student by student.Last[0]; Note that the type of the query has now changed. It now produces a sequence of groups that have a char type as a key, and a sequence of Student objects. 12345678910// studentGroup is a IGrouping&lt;char, Student&gt;foreach (var studentGroup in studentQuery2)&#123; Console.WriteLine(studentGroup.Key); foreach (Student student in studentGroup) &#123; Console.WriteLine(" &#123;0&#125;, &#123;1&#125;", student.Last, student.First); &#125;&#125; To make the variables implicitly typedYou can write the same query and foreach loop much more conveniently by using var. The var keyword does not change the types of your objects; it just instructs the compiler to infer the types.Note that in the inner foreach loop, the iteration variable is still typed as Student, and the query works just as before.12345678910111213var studentQuery3 = from student in students group student by student.Last[0];foreach (var groupOfStudents in studentQuery3)&#123; Console.WriteLine(groupOfStudents.Key); foreach (var student in groupOfStudents) &#123; Console.WriteLine(" &#123;0&#125;, &#123;1&#125;", student.Last, student.First); &#125;&#125; To order the groups by their key valueWhen you run the previous query, you notice that the groups are not in alphabetical order. To change this, you must provide an orderby clause after the group clause.But to use an orderby clause, you first need an identifier that serves as a reference to the groups created by the group clause. You provide the identifier by using the into keyword, as follows:123456789101112131415var studentQuery4 = from student in students group student by student.Last[0] into studentGroup orderby studentGroup.Key select studentGroup;foreach (var groupOfStudents in studentQuery4)&#123; Console.WriteLine(groupOfStudents.Key); foreach (var student in groupOfStudents) &#123; Console.WriteLine(" &#123;0&#125;, &#123;1&#125;", student.Last, student.First); &#125;&#125; To introduce an identifier by using letYou can use the let keyword to introduce an identifier for any expression result in the query expression. This identifier can be a convenience, as in the following example, or it can enhance performance by storing the results of an expression so that it does not have to be calculated multiple times.123456789101112131415// studentQuery5 is an IEnumerable&lt;string&gt;// This query returns those students whose// first test score was higher than their// average score.var studentQuery5 = from student in students let totalScore = student.Scores[0] + student.Scores[1] + student.Scores[2] + student.Scores[3] where totalScore / 4 &lt; student.Scores[0] select student.Last + " " + student.First;foreach (string s in studentQuery5)&#123; Console.WriteLine(s);&#125; To use method syntax in a query expressionThe following code calculates the total score for each Student in the source sequence, and then calls the Average() method on the results of that query to calculate the average score of the class.1234567891011var studentQuery6 = from student in students let totalScore = student.Scores[0] + student.Scores[1] + student.Scores[2] + student.Scores[3] select totalScore;double averageScore = studentQuery6.Average();Console.WriteLine("Class average score = &#123;0&#125;", averageScore);// Output:// Class average score = 334.166666666667 To transform or project in the select clause It is very common for a query to produce a sequence whose elements differ from the elements in the source sequences. Delete or comment out your previous query and execution loop, and replace it with the following code. Note that the query returns a sequence of strings (not Students), and this fact is reflected in the foreach loop. 12345678910IEnumerable&lt;string&gt; studentQuery7 = from student in students where student.Last == "Garcia" select student.First;Console.WriteLine("The Garcias in the class are:");foreach (string s in studentQuery7)&#123; Console.WriteLine(s);&#125; Code earlier in this walkthrough indicated that the average class score is approximately 334. To produce a sequence of Students whose total score is greater than the class average, together with their Student ID, you can use an anonymous type in the select statement: 1234567891011var studentQuery8 = from student in students let x = student.Scores[0] + student.Scores[1] + student.Scores[2] + student.Scores[3] where x &gt; averageScore select new &#123; id = student.ID, score = x &#125;;foreach (var item in studentQuery8)&#123; Console.WriteLine("Student ID: &#123;0&#125;, Score: &#123;1&#125;", item.id, item.score);&#125; ReferencesLanguage Integrated QueryLINQ FundamentalLINQ TutorialGetting Started with LINQ in C#]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>LINQ</tag>
        <tag>C#</tag>
        <tag>ASP.NET</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebSocket Tutorial]]></title>
    <url>%2F2018%2F02%2F12%2FWebSocket%2F</url>
    <content type="text"><![CDATA[Why WebSocketMany people might have the doubt, why do we need WebSocket when we already have HTTP protocal. The answer is simple: with HTTP communication could only be initiated by client. For example, if we want to know about the weather today, we can only send request from client to server, then the server responses with the search result. It’s not applicable for server to push information to client automatically. Under the condition that client need the real-time status of server, it would be difficult for traditional HTTP protocal to achieve that. One commonly used solution is called polling. But polling is inefficient which creates a large amount of requests. Thus, WebSocket is created to solve the problem above. OverviewWeb Sockets is a next-generation bidirectional communication technology for web applications which operates over a single socket and is exposed via a JavaScript interface in HTML 5 compliant browsers. Once you get a Web Socket connection with the web server, you can send data from browser to server by calling a send() method, and receive data from server to browser by an onmessage event handler. Following is the API which creates a new WebSocket object. 1var Socket = new WebSocket(url, [protocal] ); Here first argument, url, specifies the URL to which to connect. The second attribute, protocol is optional, and if present, specifies a sub-protocol that the server must support for the connection to be successful. WebSocket AttributesFollowing are the attribute of WebSocket object. Assuming we created Socket object as mentioned above − Attribute Description Socket.readyState The readonly attribute readyState represents the state of the connection. It can have the following values:A value of 0 indicates that the connection has not yet been established.A value of 1 indicates that the connection is established and communication is possible. A value of 2 indicates that the connection is going through the closing handshake. A value of 3 indicates that the connection has been closed or could not be opened. Socket.bufferedAmount The readonly attribute bufferedAmount represents the number of bytes of UTF-8 text that have been queued using send() method. WebSocket EventsFollowing are the events associated with WebSocket object. Assuming we created Socket object as mentioned above: Event Event Handler Description open Socket.onopen This event occurs when socket connection is established. message Socket.onmessage This event occurs when client receives data from server. error Socket.onerror This event occurs when there is any error in communication. close Socket.onclose This event occurs when connection is closed. WebSocket MethodsFollowing are the methods associated with WebSocket object. Assuming we created Socket object as mentioned above: Method Description Socket.send() The send(data) method transmits data using the connection. Socket.close() The close() method would be used to terminate any existing connection. WebSocket ExampleA WebSocket is a standard bidirectional TCP socket between the client and the server. The socket starts out as a HTTP connection and then “Upgrades” to a TCP socket after a HTTP handshake. After the handshake, either side can send data. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt;&lt;script type="text/javascript"&gt;function WebSocketTest()&#123;if ("WebSocket" in window)&#123;alert("WebSocket is supported by your Browser!");// Let us open a web socketvar ws = new WebSocket("ws://localhost:9998/echo");ws.onopen = function()&#123;// Web Socket is connected, send data using send()ws.send("Message to send");alert("Message is sent...");&#125;;ws.onmessage = function (evt)&#123;var received_msg = evt.data;alert("Message is received...");&#125;;ws.onclose = function()&#123;// websocket is closed.alert("Connection is closed...");&#125;;window.onbeforeunload = function(event) &#123;socket.close();&#125;;&#125;else&#123;// The browser doesn't support WebSocketalert("WebSocket NOT supported by your Browser!");&#125;&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="sse"&gt;&lt;a href="javascript:WebSocketTest()"&gt;Run WebSocket&lt;/a&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; ReferencesIntroducing WebSockets: Bringing Sockets to the WebWebSockets - Send &amp; Receive MessagesHTML5 - WebSocketsPush technologyWebSockets vs REST: Understanding the DifferenceWebSockets Tutorial]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>WebSocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Building a cat classifier with logistic regression from scratch]]></title>
    <url>%2F2018%2F02%2F09%2Fbuilding-a-cat-classifier-with-logistic-regression%2F</url>
    <content type="text"><![CDATA[GoalBuild the general architecture of a learning algorithm, including: Initializing parameters Calculating the cost function and its gradient Using an optimization algorithm (gradient descent) Gather all three functions above into a main model function, in the right order. PackagesFirst, let’s run the cell below to import all the packages. numpy is the fundamental package for scientific computing with Python. h5py is a common package to interact with a dataset that is stored on an H5 file. matplotlib is a famous library to plot graphs in Python. PIL and scipy are used here to test your model with your own picture at the end. 123456789import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset%matplotlib inline Overview of the Problem setThe dataset we will use contains: a training set of m_train images labeled as cat (y=1) or non-cat (y=0) a test set of m_test images labeled as cat or non-cat each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px). First we import the data from file 12# Loading the data (cat/non-cat)train_set_x_orig, train_set_y, test_set_x_orig, test_set_y Now we show information of data, each image is of shape (64, 64, 3) showing it’s 64*64, RGB. 12345678print ("Number of training examples: m_train = " + str(m_train))print ("Number of testing examples: m_test = " + str(m_test))print ("Height/Width of each image: num_px = " + str(num_px))print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")print ("train_set_x shape: " + str(train_set_x_orig.shape))print ("train_set_y shape: " + str(train_set_y.shape))print ("test_set_x shape: " + str(test_set_x_orig.shape))print ("test_set_y shape: " + str(test_set_y.shape)) Output:Number of training examples: m_train = 209Number of testing examples: m_test = 50Height/Width of each image: num_px = 64Each image is of size: (64, 64, 3)train_set_x shape: (209, 64, 64, 3)train_set_y shape: (1, 209)test_set_x shape: (50, 64, 64, 3)test_set_y shape: (1, 50) Then we reshape the training and test dataset to column vector:1234print ("train_set_x_flatten shape: " + str(train_set_x_flatten.shape))print ("train_set_y shape: " + str(train_set_y.shape))print ("test_set_x_flatten shape: " + str(test_set_x_flatten.shape))print ("test_set_y shape: " + str(test_set_y.shape)) Output:train_set_x_flatten shape: (12288, 209)train_set_y shape: (1, 209)test_set_x_flatten shape: (12288, 50)test_set_y shape: (1, 50) At last, let’s standardize our dataset:12train_set_x = train_set_x_flatten/255.test_set_x = test_set_x_flatten/255. Mathematical expression of the algorithm Step Initialize the parameters of the model Learn the parameters for the model by minimizing the cost Use the learned parameters to make predictions (on the test set) Analyse the results and conclude For one example $x^{(i)}$:$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$$$ \mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \log(a^{(i)}) - (1-y^{(i)} ) \log(1-a^{(i)})\tag{3}$$ The cost is then computed by summing over all training examples:$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$ Building the parts of our algorithm StepThe main steps for building a Neural Network are: Define the model structure (such as number of input features) Initialize the model’s parameters Loop: Calculate current loss (forward propagation) Calculate current gradient (backward propagation) Update parameters (gradient descent) Forward and Backward propagationForward Propagation: You get X You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, …, a^{(m-1)}, a^{(m)})$ You calculate the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$ Here are the two formulas you will be using: $$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$$ $$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$$ Here is the Forward-propagation function:1234567891011121314151617181920212223242526272829303132333435def propagate(w, b, X, Y):"""Implement the cost function and its gradient for the propagation explained aboveArguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of size (num_px * num_px * 3, number of examples)Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)Return:cost -- negative log-likelihood cost for logistic regressiondw -- gradient of the loss with respect to w, thus same shape as wdb -- gradient of the loss with respect to b, thus same shape as b"""m = X.shape[1]# FORWARD PROPAGATION (FROM X TO COST)# compute activation (1, number of examples)A = sigmoid(np.dot(w.T, X) + b)# compute costcost = -1.0/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))# BACKWARD PROPAGATION (TO FIND GRAD)# compute gradientdw = 1.0/m*np.dot(X, (A-Y).T)db = 1.0/m*np.sum(A-Y)cost = np.squeeze(cost)grads = &#123;"dw": dw,"db": db&#125;return grads, cost 12345w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])grads, cost = propagate(w, b, X, Y)print (&quot;dw = &quot; + str(grads[&quot;dw&quot;]))print (&quot;db = &quot; + str(grads[&quot;db&quot;]))print (&quot;cost = &quot; + str(cost)) Output:dw = [[ 0.99845601][ 2.39507239]]db = 0.00145557813678cost = 5.80154531939 Optimization123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):&quot;&quot;&quot;This function optimizes w and b by running a gradient descent algorithmArguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of shape (num_px * num_px * 3, number of examples)Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)num_iterations -- number of iterations of the optimization looplearning_rate -- learning rate of the gradient descent update ruleprint_cost -- True to print the loss every 100 stepsReturns:params -- dictionary containing the weights w and bias bgrads -- dictionary containing the gradients of the weights and bias with respect to the cost functioncosts -- list of all the costs computed during the optimization, this will be used to plot the learning curve.Tips:1) Calculate the cost and the gradient for the current parameters. Use propagate().2) Update the parameters using gradient descent rule for w and b.&quot;&quot;&quot;costs = []for i in range(num_iterations):# Cost and gradient calculationgrads, cost = propagate(w, b, X, Y)# Retrieve derivatives from gradsdw = grads[&quot;dw&quot;]db = grads[&quot;db&quot;]# update rulew = w - learning_rate * dwb = b - learning_rate * db# Record the costsif i % 100 == 0:costs.append(cost)# Print the cost every 100 training examplesif print_cost and i % 100 == 0:print (&quot;Cost after iteration %i: %f&quot; %(i, cost))params = &#123;&quot;w&quot;: w,&quot;b&quot;: b&#125;grads = &#123;&quot;dw&quot;: dw,&quot;db&quot;: db&#125;return params, grads, costs 123456params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)print (&quot;w = &quot; + str(params[&quot;w&quot;]))print (&quot;b = &quot; + str(params[&quot;b&quot;]))print (&quot;dw = &quot; + str(grads[&quot;dw&quot;]))print (&quot;db = &quot; + str(grads[&quot;db&quot;])) Output:w = [[ 0.19033591][ 0.12259159]]b = 1.92535983008dw = [[ 0.67752042][ 1.41625495]]db = 0.219194504541 Prediction123456789101112131415161718192021222324252627def predict(w, b, X):&apos;&apos;&apos;Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)Arguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of size (num_px * num_px * 3, number of examples)Returns:Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X&apos;&apos;&apos;m = X.shape[1]Y_prediction = np.zeros((1, m))w = w.reshape(X.shape[0], 1)# Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the pictureA = sigmoid(np.dot(w.T, X) + b)for i in range(A.shape[1]):# Convert probabilities a[0,i] to actual predictions p[0,i]Y_prediction[0, i] = 1 if A[0, i] &gt; 0.5 else 0assert(Y_prediction.shape == (1, m))return Y_prediction Step1.Initialize (w,b) Optimize the loss iteratively to learn parameters (w,b): computing the cost and its gradient updating the parameters using gradient descent Use the learned (w,b) to predict the labels for a given set of examples Merge all functions Implement the model function: Y_prediction for your predictions on the test set Y_prediction_train for your predictions on the train set w, costs, grads for the outputs of optimize() 123456789101112131415161718192021222324252627282930313233343536373839404142434445def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):&quot;&quot;&quot;Builds the logistic regression model by calling the function implemented previouslyArguments:X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)num_iterations -- hyperparameter representing the number of iterations to optimize the parameterslearning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()print_cost -- Set to true to print the cost every 100 iterationsReturns:d -- dictionary containing information about the model.&quot;&quot;&quot;# initialize parameters with zerosw, b = initialize_with_zeros(X_train.shape[0])# Gradient descentparameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost);# Retrieve parameters w and b from dictionary &quot;parameters&quot;w = parameters[&quot;w&quot;]b = parameters[&quot;b&quot;]# Predict test/train set examplesY_prediction_test = predict(w, b, X_test)Y_prediction_train = predict(w, b, X_train)# Print train/test Errorsprint(&quot;train accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))print(&quot;test accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))d = &#123;&quot;costs&quot;: costs,&quot;Y_prediction_test&quot;: Y_prediction_test,&quot;Y_prediction_train&quot; : Y_prediction_train,&quot;w&quot; : w,&quot;b&quot; : b,&quot;learning_rate&quot; : learning_rate,&quot;num_iterations&quot;: num_iterations&#125;return d Run the following cell to train your model.1d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True) Output:Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872train accuracy: 99.04306220095694 %test accuracy: 70.0 % Comment:Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. Also, you see that the model is clearly overfitting the training data. ReferencesNeural Networks and Deep LearningImplementing a Neural Network from Scratch in Python – An Introductionhttps://stats.stackexchange.com/questions/211436]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Neural Network</tag>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python - Vectorization and Broadcasting]]></title>
    <url>%2F2018%2F02%2F08%2FPython-Broadcasting%2F</url>
    <content type="text"><![CDATA[VectorizationIn the course Neural Networks and Deep Learning, Andrew Ng introduces vetorization in machine learning by giving the following examples: 123456789101112131415161718192021222324import numpy as npimport timea = np.random.rand(1000000)b = np.random.rand(1000000)# vectorizationtic = time.time()c = np.dot(a,b)toc = time.time()print(c)print("Vectorization:" + str(1000*(toc-tic)) + "ms")c = 0# vectorizationtic = time.time()for i in range(1000000):c += a[i]*b[i]toc = time.time()print(c)print("For loop:" + str(1000*(toc-tic)) + "ms") The above code uses vetorization and for loop to do the same calculation. However, the for loop version cost much more time than the vectorization version. Output:249888.154501Vectorization:2.2249221801757812ms249888.154501For loop:599.0450382232666ms The above example indicates that in python it’s more efficient to use vectorization instead of for loop. This kind of technique is also called SIMD(single instruction multiple data) in CPU/GPU parallel processing. Broadcasting The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example: 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = np.array([2.0, 2.0, 2.0])&gt;&gt;&gt; a * barray([ 2., 4., 6.]) NumPy’s broadcasting rule relaxes this constraint when the arrays’ shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation: 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = 2.0&gt;&gt;&gt; a * barray([ 2., 4., 6.]) The result is equivalent to the previous example where b was an array. We can think of the scalar b being stretched during the arithmetic operation into an array with the same shape as a. The new elements in b are simply copies of the original scalar. The stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies, so that broadcasting operations are as memory and computationally efficient as possible. The code in the second example is more efficient than that in the first because broadcasting moves less memory around during the multiplication (b is a scalar rather than an array). General Broadcasting RulesWhen operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when they are equal one of them is 1 Arrays don’t have to have the same number of dimensions. For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Image (3d array): 256 x 256 x 3Scale (1d array): 3Result (3d array): 256 x 256 x 3 When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or “copied” to match the other. A (4d array): 8 x 1 x 6 x 1B (3d array): 7 x 1 x 5Result (4d array): 8 x 7 x 6 x 5 A (2d array): 5 x 4B (1d array): 1Result (2d array): 5 x 4 A (2d array): 5 x 4B (1d array): 4Result (2d array): 5 x 4 A (3d array): 15 x 3 x 5B (3d array): 15 x 1 x 5Result (3d array): 15 x 3 x 5 A (3d array): 15 x 3 x 5B (2d array): 3 x 5Result (3d array): 15 x 3 x 5 Here are examples of shapes that do not broadcast: A (1d array): 3B (1d array): 4 # trailing dimensions do not match A (2d array): 2 x 1B (3d array): 8 x 4 x 3 # second from last dimensions mismatched References Neural Networks and Deep Learning Broadcasting — NumPy v1.12 Manual]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>NumPy</tag>
        <tag>vectorization</tag>
        <tag>broadcasting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Analytics Introduction]]></title>
    <url>%2F2018%2F02%2F07%2FWeekly-Report-20180207%2F</url>
    <content type="text"><![CDATA[In order to better evaluate how the website works, I use Google Analytics to monitor statistics of the website. Here is a brief introduction of how Google Analytics works and what we can get from it. HomeIn home page there are portals to most important statistics. Real-TimeIn Real-Time page, we can monitor our website in real-time mode, which is useful for website adminstrator. In the picture we can see that there is no active users on site. AudienceIn Audience page, information of users are listed. We can see the number of users, sessions/pageviews of users and demographics of users. AquisitionIn Aquisition page, it shows how users are directed to this website, which is useful for analyzing how to get more users. BehaviorIn Behavior page, different kinds of website behaviors are presented. This section could help website administrator to evaluate the efficiency and accessibility of website. In this section I recommend Behavior Flow cause it shows how users jump between pages. ReportHere attached sample Google Analytics report for this website. Analytics Website Monitor Acquisition OverviewAnalytics Website Monitor Behavior FlowAnalytics Website Monitor LocationAnalytics Website Monitor Overview]]></content>
      <categories>
        <category>Documentation</category>
      </categories>
      <tags>
        <tag>Google Analytics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA - Mathematics (1)]]></title>
    <url>%2F2018%2F01%2F30%2FPCA-Mathematical-principle%2F</url>
    <content type="text"><![CDATA[Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. Vector representation and Dimensionality reduction of dataIn general, in data mining and machine learning, data are represented as vectors.For example:$$ (500,240,25,13,2312.15)^𝖳 $$ As we know, the scale of dimensionality of data is in proportion to complexity of machine learning algorithm, so we can reduce dimensionality to simplify calculation. Obviuosly, dimensionality reduction means loss of information. However, due to the correlation among data, we could minimize the loss in the above process, which is the purpose of PCA. Change of basesInner productInner product of two vectors is defined as: $$ (a_1,a_2,⋯,a_n)^𝖳⋅(b_1,b_2,⋯,b_n)^𝖳=a_1b_1+a_2b_2+⋯+a_nb_n $$ and $$ A⋅B=|A||B|cos(a) $$ Let |B| = 1, that is $ A⋅B=|A|cos(a) $. The above definition could be explained that the inner product of A and B equals the vector length projected from A to B. BaseTo describe a vector, we need a set of bases. Then the vector could be represented as the combination of projections to all bases. Usually, we choose (1,0) and (0,1) as bases, but actually we can choose any two linearly independent vectors as bases. For example, we can use $ (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) $ and $ (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}) $ as new bases to represent the same vector. Matrix representation of form changeFor example, we want to transform (3, 2) based on new bases $ (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) $ and $ (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}) $, which could be represented by the following form: $$\left(\begin{matrix}\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\-\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}\end{matrix}\right)\left(\begin{matrix}1 &amp; 2 &amp; 3 \\1 &amp; 2 &amp; 3\end{matrix}\right)=\left(\begin{matrix}\frac{2}{\sqrt{2}} &amp; \frac{4}{\sqrt{2}} &amp; \frac{6}{\sqrt{2}} \\0 &amp; 0 &amp; 0\end{matrix}\right)$$ In general, if we have M N-dimesion vector and want to transform them into R N-dimesion new spaces, we first construct matrix A combining R bases by row, then construct matrix B combining vectors by column. The product of A and B is the result, where the column in AB is transformed from column in A. The above shows an explanation of matrix product:product of two matrices(AB) means to transform each column vector in B to the new space with rows in A as bases.]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
        <tag>Machine Learning</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript 6 features (3)]]></title>
    <url>%2F2018%2F01%2F30%2FECMAScript-6-3%2F</url>
    <content type="text"><![CDATA[Data-Structure Set12345var s = new Set();[2,3,5,4,5,2,2].map(x =&gt; s.add(x))for (i of s) &#123;console.log(i)&#125;// 2 3 4 5 Methods: size：Returns the number of values in the Set object. add(value)：Appends a new element with the given value to the Set object. Returns the Set object. delete(value)：Removes the element associated to the value and returns the value that has(value)：Returns a boolean asserting whether an element is present with the given value in the Set object or not. clear()：Removes all elements from the Set object. Tips: Set constructor could receive an array to eliminate duplicate. 12const items = new Set([1, 2, 3, 4, 5, 5, 5, 5]);items.size // 5 For set, 5 and ‘5’ are different, and objects are always different. 1234567let set = new Set();set.add(&#123;&#125;);set.size // 1set.add(&#123;&#125;);set.size // 2 Iteration keys()：return an iterator for keys values()：return an iterator for values entries()：Returns a new Iterator object that contains an array of [value, value] for each element in the Set object, in insertion order. 12345678910111213141516171819202122let set = new Set(['red', 'green', 'blue']);for (let item of set.keys()) &#123;console.log(item);&#125;// red// green// bluefor (let item of set.values()) &#123;console.log(item);&#125;// red// green// bluefor (let item of set.entries()) &#123;console.log(item);&#125;// ["red", "red"]// ["green", "green"]// ["blue", "blue"] forEach()：Calls callbackFn once for each value present in the Set object, in insertion order.If a thisArg parameter is provided to forEach, it will be used as the this value for each callback. 12345set = new Set([1, 4, 9]);set.forEach((value, key) =&gt; console.log(key + ' : ' + value))// 1 : 1// 4 : 4// 9 : 9 MapProperties/Methodssize12345const map = new Map();map.set('foo', true);map.set('bar', false);map.size // 2 set(key, value)12345const m = new Map();m.set('edition', 6)m.set(262, 'standard')m.set(undefined, 'nah') get(key)123456const m = new Map();const hello = function() &#123;console.log('hello');&#125;;m.set(hello, 'Hello ES6!') // key is a function!m.get(hello) // Hello ES6! has(key)return a boolean shows if a key exists in the map object.12345678910const m = new Map();m.set('edition', 6);m.set(262, 'standard');m.set(undefined, 'nah');m.has('edition') // truem.has('years') // falsem.has(262) // truem.has(undefined) // true delete(key)123456const m = new Map();m.set(undefined, 'nah');m.has(undefined) // truem.delete(undefined)m.has(undefined) // false clear()clear all elements, no return value1234567let map = new Map();map.set('foo', true);map.set('bar', false);map.size // 2map.clear()map.size // 0 override:1234567const map = new Map();map.set(1, 'aaa').set(1, 'bbb');map.get(1) // "bbb" ConvertConvert Map to Array:12345const myMap = new Map().set(true, 7).set(&#123;foo: 3&#125;, ['abc']);[...myMap]// [ [ true, 7 ], [ &#123; foo: 3 &#125;, [ 'abc' ] ] ] Convert Array to Map:12345678new Map([[true, 7],[&#123;foo: 3&#125;, ['abc']]])// Map &#123;// true =&gt; 7,// Object &#123;foo: 3&#125; =&gt; ['abc']// &#125; Convert Map to Object:If keys for map are all strings, the convertion is accepted.12345678910111213function strMapToObj(strMap) &#123;let obj = Object.create(null);for (let [k,v] of strMap) &#123;obj[k] = v;&#125;return obj;&#125;const myMap = new Map().set('yes', true).set('no', false);strMapToObj(myMap)// &#123; yes: true, no: false &#125; Convert Object to Map:12345678910function objToStrMap(obj) &#123;let strMap = new Map();for (let k of Object.keys(obj)) &#123;strMap.set(k, obj[k]);&#125;return strMap;&#125;objToStrMap(&#123;yes: true, no: false&#125;)// Map &#123;"yes" =&gt; true, "no" =&gt; false&#125; Convert Map to JSON:(1) Keys are stings1234567function strMapToJson(strMap) &#123;return JSON.stringify(strMapToObj(strMap));&#125;let myMap = new Map().set('yes', true).set('no', false);strMapToJson(myMap)// '&#123;"yes":true,"no":false&#125;' (2)Keys contain non string1234567function mapToArrayJson(map) &#123;return JSON.stringify([...map]);&#125;let myMap = new Map().set(true, 7).set(&#123;foo: 3&#125;, ['abc']);mapToArrayJson(myMap)// '[[true,7],[&#123;"foo":3&#125;,["abc"]]]' Convert JSON to Map:123456function jsonToStrMap(jsonStr) &#123;return objToStrMap(JSON.parse(jsonStr));&#125;jsonToStrMap('&#123;"yes": true, "no": false&#125;')// Map &#123;'yes' =&gt; true, 'no' =&gt; false&#125; Differences between Object and Map object has a prototype so there are default keys can be bypassed using map = Object.create(null) object keys are string whereas map keys can be anything map keeps track of size use maps over objects when keys are unknown until run time use objects when there is logic that operates on individual elements WeakSet iterate through providing keys only not enumerable unique object references only accept objects Differences between Set and WeakSet WeakSets are collections of object types only references to objects in the collection are held weakly 123456789var myWeakSet = new WeakSet();var foo = &#123;&#125;;var bar = &#123;&#125;;myWeakSet.add(foo);myWeakSet.has(bar); // falsemyWeakSet.has(foo); // truemyWeakSet.delete(foo);myWeakSet.add(&#123; kobe: 24 &#125;); // But because the added object has no other references, it will not be held in the set WeakMap iterate through providing keys only not enumerable unique object or function references does not accept primitive data types as keys 12345678var myWeakMap = new WeakMap();var obj1 = &#123;&#125;;var obj2 = function()&#123;&#125;;myWeakMap.set(obj1, "cat");myWeakMap.set(obj2, 24);myWeakMap.has(obj1); // truemyWeakMap.get(obj2); // 24myWeakMap.delete(obj1);]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript 6 features (2)]]></title>
    <url>%2F2018%2F01%2F28%2FECMAScript-6-2%2F</url>
    <content type="text"><![CDATA[FunctionDefault Parameter ValuesSimple and intuitive default values for function parameters. 1234function f (x, y = 7, z = 42) &#123;return x + y + z&#125;f(1) === 50 Rest ParameterAggregation of remaining arguments into single parameter of variadic functions. 1234function f (x, y, ...a) &#123;return (x + y) * a.length&#125;f(1, 2, "hello", true, 7) === 9 Spread OperatorSpreading of elements of an iterable collection (like an array or even a string) into both literal elements and individual function parameters. 12345678910var params = [ "hello", true, 7 ]var other = [ 1, 2, ...params ] // [ 1, 2, "hello", true, 7 ]function f (x, y, ...a) &#123;return (x + y) * a.length&#125;f(1, 2, ...params) === 9var str = "foo"var chars = [ ...str ] // [ "f", "o", "o" ] Strict modeSince ES5, strict mode could be applied within function.In ES6, if a function uses default parameter value, rest parameter or spread operator, strict mode couldn’t be used in the function. Name 12function foo() &#123;&#125;foo.name // "foo" Arrow Functions1var f = v =&gt; v; equals: 123var f = function(v) &#123;return v;&#125;; If there is no arguments or more than 1 arguments, use bracket. 123456789var f = () =&gt; 5;// equalsvar f = function () &#123; return 5 &#125;;var sum = (num1, num2) =&gt; num1 + num2;// equalsvar sum = function(num1, num2) &#123;return num1 + num2;&#125;; Example of arrow function combining with rest parameters: 123456789const numbers = (...nums) =&gt; nums;numbers(1, 2, 3, 4, 5)// [1,2,3,4,5]const headAndTail = (head, ...tail) =&gt; [head, tail];headAndTail(1, 2, 3, 4, 5)// [1,[2,3,4,5]] Tail CallDefinition: call one function at the last step of another function.123function f(x)&#123;return g(x);&#125; Tail Recursion A recursive function is tail recursive when recursive call is the last thing executed by the function. 123456function factorial(n, total) &#123;if (n === 1) return total;return factorial(n - 1, n * total);&#125;factorial(5, 1) // 120 The tail recursive functions considered better than non tail recursive functions as tail-recursion can be optimized by compiler. The idea used by compilers to optimize tail-recursive functions is simple, since the recursive call is the last statement, there is nothing left to do in the current function, so saving the current function’s stack frame is of no use (See this for more details). Example: (Fibonacci) Non tail recursive: 123456789function Fibonacci (n) &#123;if ( n &lt;= 1 ) &#123;return 1&#125;;return Fibonacci(n - 1) + Fibonacci(n - 2);&#125;Fibonacci(10) // 89Fibonacci(100) // overflowFibonacci(500) // overflow Tail recursive: 123456789function Fibonacci2 (n , ac1 = 1 , ac2 = 1) &#123;if( n &lt;= 1 ) &#123;return ac2&#125;;return Fibonacci2 (n - 1, ac2, ac1 + ac2);&#125;Fibonacci2(100) // 573147844013817200000Fibonacci2(1000) // 7.0330367711422765e+208Fibonacci2(10000) // Infinity]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript 6 features (1)]]></title>
    <url>%2F2018%2F01%2F28%2FECMAScript-6-1%2F</url>
    <content type="text"><![CDATA[constSupport for constants, variables which cannot be re-assigned new content. Notice: this only makes the variable itself immutable, not its assigned content (for instance, in case the content is an object, this means the object itself can still be altered). 12345678const foo = &#123;&#125;;// add an attribute to foo, successfoo.prop = 123;foo.prop // 123// assign foo to another object, errorfoo = &#123;&#125;; // TypeError: "foo" is read-only letBlock-scoped variables (and constants) without hoisting. ‘let’ only validates in scope: 123456789101112131415&#123;let a = 10;var b = 1;&#125;a // ReferenceError: a is not defined.b // 1which is useful in for loop:for (let i = 0; i &lt; 10; i++) &#123;// ...&#125;console.log(i);// ReferenceError: i is not defined Temporal dead zone (TDZ)If a variable is declared by ‘let’ in a block scope, it’s binding to the block scope and refrain from outside. 1234567891011if (true) &#123;// TDZ starttmp = 'abc'; // ReferenceErrorconsole.log(tmp); // ReferenceErrorlet tmp; // TDZ endconsole.log(tmp); // undefinedtmp = 123;console.log(tmp); // 123&#125; Repeat declaration is not allowed 1234567891011// errorfunction func() &#123;let a = 10;var a = 1;&#125;// errorfunction func() &#123;let a = 10;let a = 1;&#125; ES6 has 6 methods of variable declarationES5 has two methods of variable declaration: var, function;ES6 has 4 more methods: let, const, import, class.]]></content>
      <categories>
        <category>Web Development</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello world]]></title>
    <url>%2F2018%2F01%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hello world This website is Yu Liu’s homepage, built by Hexo+Github pages. Contents are written with Markdown syntax. This website is for: 1. Share idea “Do not go gentle into that good night” 2. Code review1234567@requires_authorizationclass SomeClass:passif __name__ == '__main__':# A commentprint 'hello world' 3. Photography 4. Schedule and plan Set up TensorFlow environment Build personal website Solve algorithm problems Learn ASP.NET MVC Framework]]></content>
      <categories>
        <category>Documentation</category>
      </categories>
  </entry>
</search>
