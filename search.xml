<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[WebSocket]]></title>
    <url>%2F2018%2F02%2F12%2FWebSocket%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Building a cat classifier with logistic regression from scratch]]></title>
    <url>%2F2018%2F02%2F09%2Fbuilding-a-cat-classifier-with-logistic-regression%2F</url>
    <content type="text"><![CDATA[GoalBuild the general architecture of a learning algorithm, including: Initializing parameters Calculating the cost function and its gradient Using an optimization algorithm (gradient descent) Gather all three functions above into a main model function, in the right order. PackagesFirst, letâ€™s run the cell below to import all the packages. numpy is the fundamental package for scientific computing with Python. h5py is a common package to interact with a dataset that is stored on an H5 file. matplotlib is a famous library to plot graphs in Python. PIL and scipy are used here to test your model with your own picture at the end. 123456789import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset%matplotlib inline Overview of the Problem setThe dataset we will use contains: a training set of m_train images labeled as cat (y=1) or non-cat (y=0) a test set of m_test images labeled as cat or non-cat each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px). First we import the data from file 12# Loading the data (cat/non-cat)train_set_x_orig, train_set_y, test_set_x_orig, test_set_y Now we show information of data, each image is of shape (64, 64, 3) showing itâ€™s 64*64, RGB. 12345678print ("Number of training examples: m_train = " + str(m_train))print ("Number of testing examples: m_test = " + str(m_test))print ("Height/Width of each image: num_px = " + str(num_px))print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")print ("train_set_x shape: " + str(train_set_x_orig.shape))print ("train_set_y shape: " + str(train_set_y.shape))print ("test_set_x shape: " + str(test_set_x_orig.shape))print ("test_set_y shape: " + str(test_set_y.shape)) Output:Number of training examples: m_train = 209Number of testing examples: m_test = 50Height/Width of each image: num_px = 64Each image is of size: (64, 64, 3)train_set_x shape: (209, 64, 64, 3)train_set_y shape: (1, 209)test_set_x shape: (50, 64, 64, 3)test_set_y shape: (1, 50) Then we reshape the training and test dataset to column vector:1234print ("train_set_x_flatten shape: " + str(train_set_x_flatten.shape))print ("train_set_y shape: " + str(train_set_y.shape))print ("test_set_x_flatten shape: " + str(test_set_x_flatten.shape))print ("test_set_y shape: " + str(test_set_y.shape)) Output:train_set_x_flatten shape: (12288, 209)train_set_y shape: (1, 209)test_set_x_flatten shape: (12288, 50)test_set_y shape: (1, 50) At last, letâ€™s standardize our dataset:12train_set_x = train_set_x_flatten/255.test_set_x = test_set_x_flatten/255. Mathematical expression of the algorithm Step Initialize the parameters of the model Learn the parameters for the model by minimizing the cost Use the learned parameters to make predictions (on the test set) Analyse the results and conclude For one example $x^{(i)}$:$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$$$ \mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \log(a^{(i)}) - (1-y^{(i)} ) \log(1-a^{(i)})\tag{3}$$ The cost is then computed by summing over all training examples:$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$ Building the parts of our algorithm StepThe main steps for building a Neural Network are: Define the model structure (such as number of input features) Initialize the modelâ€™s parameters Loop: Calculate current loss (forward propagation) Calculate current gradient (backward propagation) Update parameters (gradient descent) Forward and Backward propagationForward Propagation: You get X You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, â€¦, a^{(m-1)}, a^{(m)})$ You calculate the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$ Here are the two formulas you will be using: $$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$$ $$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$$ Here is the Forward-propagation function:1234567891011121314151617181920212223242526272829303132333435def propagate(w, b, X, Y):"""Implement the cost function and its gradient for the propagation explained aboveArguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of size (num_px * num_px * 3, number of examples)Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)Return:cost -- negative log-likelihood cost for logistic regressiondw -- gradient of the loss with respect to w, thus same shape as wdb -- gradient of the loss with respect to b, thus same shape as b"""m = X.shape[1]# FORWARD PROPAGATION (FROM X TO COST)# compute activation (1, number of examples)A = sigmoid(np.dot(w.T, X) + b)# compute costcost = -1.0/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))# BACKWARD PROPAGATION (TO FIND GRAD)# compute gradientdw = 1.0/m*np.dot(X, (A-Y).T)db = 1.0/m*np.sum(A-Y)cost = np.squeeze(cost)grads = &#123;"dw": dw,"db": db&#125;return grads, cost 12345w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])grads, cost = propagate(w, b, X, Y)print (&quot;dw = &quot; + str(grads[&quot;dw&quot;]))print (&quot;db = &quot; + str(grads[&quot;db&quot;]))print (&quot;cost = &quot; + str(cost)) Output:dw = [[ 0.99845601][ 2.39507239]]db = 0.00145557813678cost = 5.80154531939 Optimization123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):&quot;&quot;&quot;This function optimizes w and b by running a gradient descent algorithmArguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of shape (num_px * num_px * 3, number of examples)Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)num_iterations -- number of iterations of the optimization looplearning_rate -- learning rate of the gradient descent update ruleprint_cost -- True to print the loss every 100 stepsReturns:params -- dictionary containing the weights w and bias bgrads -- dictionary containing the gradients of the weights and bias with respect to the cost functioncosts -- list of all the costs computed during the optimization, this will be used to plot the learning curve.Tips:1) Calculate the cost and the gradient for the current parameters. Use propagate().2) Update the parameters using gradient descent rule for w and b.&quot;&quot;&quot;costs = []for i in range(num_iterations):# Cost and gradient calculationgrads, cost = propagate(w, b, X, Y)# Retrieve derivatives from gradsdw = grads[&quot;dw&quot;]db = grads[&quot;db&quot;]# update rulew = w - learning_rate * dwb = b - learning_rate * db# Record the costsif i % 100 == 0:costs.append(cost)# Print the cost every 100 training examplesif print_cost and i % 100 == 0:print (&quot;Cost after iteration %i: %f&quot; %(i, cost))params = &#123;&quot;w&quot;: w,&quot;b&quot;: b&#125;grads = &#123;&quot;dw&quot;: dw,&quot;db&quot;: db&#125;return params, grads, costs 123456params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)print (&quot;w = &quot; + str(params[&quot;w&quot;]))print (&quot;b = &quot; + str(params[&quot;b&quot;]))print (&quot;dw = &quot; + str(grads[&quot;dw&quot;]))print (&quot;db = &quot; + str(grads[&quot;db&quot;])) Output:w = [[ 0.19033591][ 0.12259159]]b = 1.92535983008dw = [[ 0.67752042][ 1.41625495]]db = 0.219194504541 Prediction123456789101112131415161718192021222324252627def predict(w, b, X):&apos;&apos;&apos;Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)Arguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of size (num_px * num_px * 3, number of examples)Returns:Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X&apos;&apos;&apos;m = X.shape[1]Y_prediction = np.zeros((1, m))w = w.reshape(X.shape[0], 1)# Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the pictureA = sigmoid(np.dot(w.T, X) + b)for i in range(A.shape[1]):# Convert probabilities a[0,i] to actual predictions p[0,i]Y_prediction[0, i] = 1 if A[0, i] &gt; 0.5 else 0assert(Y_prediction.shape == (1, m))return Y_prediction Step1.Initialize (w,b) Optimize the loss iteratively to learn parameters (w,b): computing the cost and its gradient updating the parameters using gradient descent Use the learned (w,b) to predict the labels for a given set of examples Merge all functions Implement the model function: Y_prediction for your predictions on the test set Y_prediction_train for your predictions on the train set w, costs, grads for the outputs of optimize() 123456789101112131415161718192021222324252627282930313233343536373839404142434445def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):&quot;&quot;&quot;Builds the logistic regression model by calling the function implemented previouslyArguments:X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)num_iterations -- hyperparameter representing the number of iterations to optimize the parameterslearning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()print_cost -- Set to true to print the cost every 100 iterationsReturns:d -- dictionary containing information about the model.&quot;&quot;&quot;# initialize parameters with zerosw, b = initialize_with_zeros(X_train.shape[0])# Gradient descentparameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost);# Retrieve parameters w and b from dictionary &quot;parameters&quot;w = parameters[&quot;w&quot;]b = parameters[&quot;b&quot;]# Predict test/train set examplesY_prediction_test = predict(w, b, X_test)Y_prediction_train = predict(w, b, X_train)# Print train/test Errorsprint(&quot;train accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))print(&quot;test accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))d = &#123;&quot;costs&quot;: costs,&quot;Y_prediction_test&quot;: Y_prediction_test,&quot;Y_prediction_train&quot; : Y_prediction_train,&quot;w&quot; : w,&quot;b&quot; : b,&quot;learning_rate&quot; : learning_rate,&quot;num_iterations&quot;: num_iterations&#125;return d Run the following cell to train your model.1d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True) Output:Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872train accuracy: 99.04306220095694 %test accuracy: 70.0 % Comment:Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. Also, you see that the model is clearly overfitting the training data. ReferencesNeural Networks and Deep LearningImplementing a Neural Network from Scratch in Python â€“ An Introductionhttps://stats.stackexchange.com/questions/211436]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
        <tag>Logistic Regression</tag>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python - Vectorization and Broadcasting]]></title>
    <url>%2F2018%2F02%2F08%2FPython-Broadcasting%2F</url>
    <content type="text"><![CDATA[VectorizationIn the course Neural Networks and Deep Learning, Andrew Ng introduces vetorization in machine learning by giving the following examples: 123456789101112131415161718192021222324import numpy as npimport timea = np.random.rand(1000000)b = np.random.rand(1000000)# vectorizationtic = time.time()c = np.dot(a,b)toc = time.time()print(c)print("Vectorization:" + str(1000*(toc-tic)) + "ms")c = 0# vectorizationtic = time.time()for i in range(1000000):c += a[i]*b[i]toc = time.time()print(c)print("For loop:" + str(1000*(toc-tic)) + "ms") The above code uses vetorization and for loop to do the same calculation. However, the for loop version cost much more time than the vectorization version. Output:249888.154501Vectorization:2.2249221801757812ms249888.154501For loop:599.0450382232666ms The above example indicates that in python itâ€™s more efficient to use vectorization instead of for loop. This kind of technique is also called SIMD(single instruction multiple data) in CPU/GPU parallel processing. Broadcasting The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is â€œbroadcastâ€ across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example: 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = np.array([2.0, 2.0, 2.0])&gt;&gt;&gt; a * barray([ 2., 4., 6.]) NumPyâ€™s broadcasting rule relaxes this constraint when the arraysâ€™ shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation: 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = 2.0&gt;&gt;&gt; a * barray([ 2., 4., 6.]) The result is equivalent to the previous example where b was an array. We can think of the scalar b being stretched during the arithmetic operation into an array with the same shape as a. The new elements in b are simply copies of the original scalar. The stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies, so that broadcasting operations are as memory and computationally efficient as possible. The code in the second example is more efficient than that in the first because broadcasting moves less memory around during the multiplication (b is a scalar rather than an array). General Broadcasting RulesWhen operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when they are equal one of them is 1 Arrays donâ€™t have to have the same number of dimensions. For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Image (3d array): 256 x 256 x 3Scale (1d array): 3Result (3d array): 256 x 256 x 3 When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or â€œcopiedâ€ to match the other. A (4d array): 8 x 1 x 6 x 1B (3d array): 7 x 1 x 5Result (4d array): 8 x 7 x 6 x 5 A (2d array): 5 x 4B (1d array): 1Result (2d array): 5 x 4 A (2d array): 5 x 4B (1d array): 4Result (2d array): 5 x 4 A (3d array): 15 x 3 x 5B (3d array): 15 x 1 x 5Result (3d array): 15 x 3 x 5 A (3d array): 15 x 3 x 5B (2d array): 3 x 5Result (3d array): 15 x 3 x 5 Here are examples of shapes that do not broadcast: A (1d array): 3B (1d array): 4 # trailing dimensions do not match A (2d array): 2 x 1B (3d array): 8 x 4 x 3 # second from last dimensions mismatched References Neural Networks and Deep Learning Broadcasting â€” NumPy v1.12 Manual]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NumPy</tag>
        <tag>vectorization</tag>
        <tag>broadcasting</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Analytics Introduction]]></title>
    <url>%2F2018%2F02%2F07%2FWeekly-Report-20180207%2F</url>
    <content type="text"><![CDATA[In order to better evaluate how the website works, I use Google Analytics to monitor statistics of the website. Here is a brief introduction of how Google Analytics works and what we can get from it. HomeIn home page there are portals to most important statistics. Real-TimeIn Real-Time page, we can monitor our website in real-time mode, which is useful for website adminstrator. In the picture we can see that there is no active users on site. AudienceIn Audience page, information of users are listed. We can see the number of users, sessions/pageviews of users and demographics of users. AquisitionIn Aquisition page, it shows how users are directed to this website, which is useful for analyzing how to get more users. BehaviorIn Behavior page, different kinds of website behaviors are presented. This section could help website administrator to evaluate the efficiency and accessibility of website. In this section I recommend Behavior Flow cause it shows how users jump between pages. ReportHere attached sample Google Analytics report for this website. Analytics Website Monitor Acquisition OverviewAnalytics Website Monitor Behavior FlowAnalytics Website Monitor LocationAnalytics Website Monitor Overview]]></content>
      <categories>
        <category>Documentation</category>
      </categories>
      <tags>
        <tag>Google Analytics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA - Mathematics (1)]]></title>
    <url>%2F2018%2F01%2F30%2FPCA-Mathematical-principle%2F</url>
    <content type="text"><![CDATA[Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. Vector representation and Dimensionality reduction of dataIn general, in data mining and machine learning, data are represented as vectors.For example:$$ (500,240,25,13,2312.15)^ð–³ $$ As we know, the scale of dimensionality of data is in proportion to complexity of machine learning algorithm, so we can reduce dimensionality to simplify calculation. Obviuosly, dimensionality reduction means loss of information. However, due to the correlation among data, we could minimize the loss in the above process, which is the purpose of PCA. Change of basesInner productInner product of two vectors is defined as: $$ (a_1,a_2,â‹¯,a_n)^ð–³â‹…(b_1,b_2,â‹¯,b_n)^ð–³=a_1b_1+a_2b_2+â‹¯+a_nb_n $$ and $$ Aâ‹…B=|A||B|cos(a) $$ Let |B| = 1, that is $ Aâ‹…B=|A|cos(a) $. The above definition could be explained that the inner product of A and B equals the vector length projected from A to B. BaseTo describe a vector, we need a set of bases. Then the vector could be represented as the combination of projections to all bases. Usually, we choose (1,0) and (0,1) as bases, but actually we can choose any two linearly independent vectors as bases. For example, we can use $ (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) $ and $ (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}) $ as new bases to represent the same vector. Matrix representation of form changeFor example, we want to transform (3, 2) based on new bases $ (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) $ and $ (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}) $, which could be represented by the following form: $$\left(\begin{matrix}\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\-\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}\end{matrix}\right)\left(\begin{matrix}1 &amp; 2 &amp; 3 \\1 &amp; 2 &amp; 3\end{matrix}\right)=\left(\begin{matrix}\frac{2}{\sqrt{2}} &amp; \frac{4}{\sqrt{2}} &amp; \frac{6}{\sqrt{2}} \\0 &amp; 0 &amp; 0\end{matrix}\right)$$ In general, if we have M N-dimesion vector and want to transform them into R N-dimesion new spaces, we first construct matrix A combining R bases by row, then construct matrix B combining vectors by column. The product of A and B is the result, where the column in AB is transformed from column in A. The above shows an explanation of matrix product:product of two matrices(AB) means to transform each column vector in B to the new space with rows in A as bases.]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Mathematics</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript 6 features (3)]]></title>
    <url>%2F2018%2F01%2F30%2FECMAScript-6-3%2F</url>
    <content type="text"><![CDATA[Data-Structure Set12345var s = new Set();[2,3,5,4,5,2,2].map(x =&gt; s.add(x))for (i of s) &#123;console.log(i)&#125;// 2 3 4 5 Methods: sizeï¼šReturns the number of values in the Set object. add(value)ï¼šAppends a new element with the given value to the Set object. Returns the Set object. delete(value)ï¼šRemoves the element associated to the value and returns the value that has(value)ï¼šReturns a boolean asserting whether an element is present with the given value in the Set object or not. clear()ï¼šRemoves all elements from the Set object. Tips: Set constructor could receive an array to eliminate duplicate. 12const items = new Set([1, 2, 3, 4, 5, 5, 5, 5]);items.size // 5 For set, 5 and â€˜5â€™ are different, and objects are always different. 1234567let set = new Set();set.add(&#123;&#125;);set.size // 1set.add(&#123;&#125;);set.size // 2 Iteration keys()ï¼šreturn an iterator for keys values()ï¼šreturn an iterator for values entries()ï¼šReturns a new Iterator object that contains an array of [value, value] for each element in the Set object, in insertion order. 12345678910111213141516171819202122let set = new Set(['red', 'green', 'blue']);for (let item of set.keys()) &#123;console.log(item);&#125;// red// green// bluefor (let item of set.values()) &#123;console.log(item);&#125;// red// green// bluefor (let item of set.entries()) &#123;console.log(item);&#125;// ["red", "red"]// ["green", "green"]// ["blue", "blue"] forEach()ï¼šCalls callbackFn once for each value present in the Set object, in insertion order.If a thisArg parameter is provided to forEach, it will be used as the this value for each callback. 12345set = new Set([1, 4, 9]);set.forEach((value, key) =&gt; console.log(key + ' : ' + value))// 1 : 1// 4 : 4// 9 : 9 MapProperties/Methodssize12345const map = new Map();map.set('foo', true);map.set('bar', false);map.size // 2 set(key, value)12345const m = new Map();m.set('edition', 6)m.set(262, 'standard')m.set(undefined, 'nah') get(key)123456const m = new Map();const hello = function() &#123;console.log('hello');&#125;;m.set(hello, 'Hello ES6!') // key is a function!m.get(hello) // Hello ES6! has(key)return a boolean shows if a key exists in the map object.12345678910const m = new Map();m.set('edition', 6);m.set(262, 'standard');m.set(undefined, 'nah');m.has('edition') // truem.has('years') // falsem.has(262) // truem.has(undefined) // true delete(key)123456const m = new Map();m.set(undefined, 'nah');m.has(undefined) // truem.delete(undefined)m.has(undefined) // false clear()clear all elements, no return value1234567let map = new Map();map.set('foo', true);map.set('bar', false);map.size // 2map.clear()map.size // 0 override:1234567const map = new Map();map.set(1, 'aaa').set(1, 'bbb');map.get(1) // "bbb" ConvertConvert Map to Array:12345const myMap = new Map().set(true, 7).set(&#123;foo: 3&#125;, ['abc']);[...myMap]// [ [ true, 7 ], [ &#123; foo: 3 &#125;, [ 'abc' ] ] ] Convert Array to Map:12345678new Map([[true, 7],[&#123;foo: 3&#125;, ['abc']]])// Map &#123;// true =&gt; 7,// Object &#123;foo: 3&#125; =&gt; ['abc']// &#125; Convert Map to Object:If keys for map are all strings, the convertion is accepted.12345678910111213function strMapToObj(strMap) &#123;let obj = Object.create(null);for (let [k,v] of strMap) &#123;obj[k] = v;&#125;return obj;&#125;const myMap = new Map().set('yes', true).set('no', false);strMapToObj(myMap)// &#123; yes: true, no: false &#125; Convert Object to Map:12345678910function objToStrMap(obj) &#123;let strMap = new Map();for (let k of Object.keys(obj)) &#123;strMap.set(k, obj[k]);&#125;return strMap;&#125;objToStrMap(&#123;yes: true, no: false&#125;)// Map &#123;"yes" =&gt; true, "no" =&gt; false&#125; Convert Map to JSON:(1) Keys are stings1234567function strMapToJson(strMap) &#123;return JSON.stringify(strMapToObj(strMap));&#125;let myMap = new Map().set('yes', true).set('no', false);strMapToJson(myMap)// '&#123;"yes":true,"no":false&#125;' (2)Keys contain non string1234567function mapToArrayJson(map) &#123;return JSON.stringify([...map]);&#125;let myMap = new Map().set(true, 7).set(&#123;foo: 3&#125;, ['abc']);mapToArrayJson(myMap)// '[[true,7],[&#123;"foo":3&#125;,["abc"]]]' Convert JSON to Map:123456function jsonToStrMap(jsonStr) &#123;return objToStrMap(JSON.parse(jsonStr));&#125;jsonToStrMap('&#123;"yes": true, "no": false&#125;')// Map &#123;'yes' =&gt; true, 'no' =&gt; false&#125; Differences between Object and Map object has a prototype so there are default keys can be bypassed using map = Object.create(null) object keys are string whereas map keys can be anything map keeps track of size use maps over objects when keys are unknown until run time use objects when there is logic that operates on individual elements WeakSet iterate through providing keys only not enumerable unique object references only accept objects Differences between Set and WeakSet WeakSets are collections of object types only references to objects in the collection are held weakly 123456789var myWeakSet = new WeakSet();var foo = &#123;&#125;;var bar = &#123;&#125;;myWeakSet.add(foo);myWeakSet.has(bar); // falsemyWeakSet.has(foo); // truemyWeakSet.delete(foo);myWeakSet.add(&#123; kobe: 24 &#125;); // But because the added object has no other references, it will not be held in the set WeakMap iterate through providing keys only not enumerable unique object or function references does not accept primitive data types as keys 12345678var myWeakMap = new WeakMap();var obj1 = &#123;&#125;;var obj2 = function()&#123;&#125;;myWeakMap.set(obj1, "cat");myWeakMap.set(obj2, 24);myWeakMap.has(obj1); // truemyWeakMap.get(obj2); // 24myWeakMap.delete(obj1);]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript 6 features (2)]]></title>
    <url>%2F2018%2F01%2F28%2FECMAScript-6-2%2F</url>
    <content type="text"><![CDATA[FunctionDefault Parameter ValuesSimple and intuitive default values for function parameters. 1234function f (x, y = 7, z = 42) &#123;return x + y + z&#125;f(1) === 50 Rest ParameterAggregation of remaining arguments into single parameter of variadic functions. 1234function f (x, y, ...a) &#123;return (x + y) * a.length&#125;f(1, 2, "hello", true, 7) === 9 Spread OperatorSpreading of elements of an iterable collection (like an array or even a string) into both literal elements and individual function parameters. 12345678910var params = [ "hello", true, 7 ]var other = [ 1, 2, ...params ] // [ 1, 2, "hello", true, 7 ]function f (x, y, ...a) &#123;return (x + y) * a.length&#125;f(1, 2, ...params) === 9var str = "foo"var chars = [ ...str ] // [ "f", "o", "o" ] Strict modeSince ES5, strict mode could be applied within function.In ES6, if a function uses default parameter value, rest parameter or spread operator, strict mode couldnâ€™t be used in the function. Name 12function foo() &#123;&#125;foo.name // "foo" Arrow Functions1var f = v =&gt; v; equals: 123var f = function(v) &#123;return v;&#125;; If there is no arguments or more than 1 arguments, use bracket. 123456789var f = () =&gt; 5;// equalsvar f = function () &#123; return 5 &#125;;var sum = (num1, num2) =&gt; num1 + num2;// equalsvar sum = function(num1, num2) &#123;return num1 + num2;&#125;; Example of arrow function combining with rest parameters: 123456789const numbers = (...nums) =&gt; nums;numbers(1, 2, 3, 4, 5)// [1,2,3,4,5]const headAndTail = (head, ...tail) =&gt; [head, tail];headAndTail(1, 2, 3, 4, 5)// [1,[2,3,4,5]] Tail CallDefinition: call one function at the last step of another function.123function f(x)&#123;return g(x);&#125; Tail Recursion A recursive function is tail recursive when recursive call is the last thing executed by the function. 123456function factorial(n, total) &#123;if (n === 1) return total;return factorial(n - 1, n * total);&#125;factorial(5, 1) // 120 The tail recursive functions considered better than non tail recursive functions as tail-recursion can be optimized by compiler. The idea used by compilers to optimize tail-recursive functions is simple, since the recursive call is the last statement, there is nothing left to do in the current function, so saving the current functionâ€™s stack frame is of no use (See this for more details). Example: (Fibonacci) Non tail recursive: 123456789function Fibonacci (n) &#123;if ( n &lt;= 1 ) &#123;return 1&#125;;return Fibonacci(n - 1) + Fibonacci(n - 2);&#125;Fibonacci(10) // 89Fibonacci(100) // overflowFibonacci(500) // overflow Tail recursive: 123456789function Fibonacci2 (n , ac1 = 1 , ac2 = 1) &#123;if( n &lt;= 1 ) &#123;return ac2&#125;;return Fibonacci2 (n - 1, ac2, ac1 + ac2);&#125;Fibonacci2(100) // 573147844013817200000Fibonacci2(1000) // 7.0330367711422765e+208Fibonacci2(10000) // Infinity]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript 6 features (1)]]></title>
    <url>%2F2018%2F01%2F28%2FECMAScript-6-1%2F</url>
    <content type="text"><![CDATA[constSupport for constants, variables which cannot be re-assigned new content. Notice: this only makes the variable itself immutable, not its assigned content (for instance, in case the content is an object, this means the object itself can still be altered). 12345678const foo = &#123;&#125;;// add an attribute to foo, successfoo.prop = 123;foo.prop // 123// assign foo to another object, errorfoo = &#123;&#125;; // TypeError: "foo" is read-only letBlock-scoped variables (and constants) without hoisting. â€˜letâ€™ only validates in scope: 123456789101112131415&#123;let a = 10;var b = 1;&#125;a // ReferenceError: a is not defined.b // 1which is useful in for loop:for (let i = 0; i &lt; 10; i++) &#123;// ...&#125;console.log(i);// ReferenceError: i is not defined Temporal dead zone (TDZ)If a variable is declared by â€˜letâ€™ in a block scope, itâ€™s binding to the block scope and refrain from outside. 1234567891011if (true) &#123;// TDZ starttmp = 'abc'; // ReferenceErrorconsole.log(tmp); // ReferenceErrorlet tmp; // TDZ endconsole.log(tmp); // undefinedtmp = 123;console.log(tmp); // 123&#125; Repeat declaration is not allowed 1234567891011// errorfunction func() &#123;let a = 10;var a = 1;&#125;// errorfunction func() &#123;let a = 10;let a = 1;&#125; ES6 has 6 methods of variable declarationES5 has two methods of variable declaration: var, function;ES6 has 4 more methods: let, const, import, class.]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello world]]></title>
    <url>%2F2018%2F01%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hello world This website is Yu Liuâ€™s homepage, built by Hexo+Github pages. Contents are written with Markdown syntax. This website is for: 1. Share idea â€œDo not go gentle into that good nightâ€ 2. Code review1234567@requires_authorizationclass SomeClass:passif __name__ == '__main__':# A commentprint 'hello world' 3. Photography 4. Schedule and plan Set up TensorFlow environment Build personal website Solve algorithm problems Learn ASP.NET MVC Framework]]></content>
      <categories>
        <category>Documentation</category>
      </categories>
  </entry>
</search>
